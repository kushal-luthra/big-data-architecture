{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Big Data Architecture Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture Topics Big Data Architectural Patterns 1. Introduction 2. Big Data Architectural Principles - A simplified data processing pipeline. - What is the temperature of my data? 3. Collect Layer - Types of data sources - Which Streaming/Message Storage should I use? - Which File/Object Storage should I use? - Data Tiering 4. Processing Layer","title":"Home"},{"location":"#welcome-to-big-data-architecture","text":"Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture","title":"Welcome to Big Data Architecture"},{"location":"#topics","text":"Big Data Architectural Patterns 1. Introduction 2. Big Data Architectural Principles - A simplified data processing pipeline. - What is the temperature of my data? 3. Collect Layer - Types of data sources - Which Streaming/Message Storage should I use? - Which File/Object Storage should I use? - Data Tiering 4. Processing Layer","title":"Topics"},{"location":"aboutme/","text":"About me Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData & Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos. He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks. Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.","title":"About Me"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/","text":"Big Data Analytics Architectural Patterns and Best Practices Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/#big-data-analytics-architectural-patterns-and-best-practices","text":"Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/","text":"There are different kind of analytical systems. Some customers are looking at doing batch-interactive analytics with their data. Others are looking at being able to to take real time data feeds, and store them for insights, and some go beyond, seking building of data models on top of it, power inferencing and do ML with that data. As we step step into the architecture discussion, we are going to discuss architecture for -> - Streaming processing - datalakes and batch interaction - machine learning. Note - you dont need to pick them right away. You can start small, architect it in a way that it enables you to build more and more features on top of it over time, incrementally. Now, lets look at different model of delivering big data services. Virtualized - eg- EC2 instances that we create & install kafka on top of it. Customer owns environment and manages it themselves. Managed Services - EMR (hadoop platform as a service), RDS - managed by AWS - customers still thinking about requirements like what configuration you need, what should be auto-scaling policy etc. Serverless/Clusterless/Containerized - Lambda, Athena, Glue - these are services that abstract out the servers away from you. This means users can focus on core use case instead of being caught up in acquiring cluster & configuring it, installing software etc. Various services - both open source and AWS based are mentioned below - Big Data Challenges Lets start with the challenge first. As a customer, one would like to have answers to questions like -> * Is there a reference architecture for my use case? * If yes, what tools should one choose? * How? * Why?","title":"Introduction"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/#big-data-challenges","text":"Lets start with the challenge first. As a customer, one would like to have answers to questions like -> * Is there a reference architecture for my use case? * If yes, what tools should one choose? * How? * Why?","title":"Big Data Challenges"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/","text":"Big Data Architectural Principles Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages. A simplified data processing pipeline Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders. What is the temperature of my data? Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#big-data-architectural-principles","text":"Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#a-simplified-data-processing-pipeline","text":"Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders.","title":"A simplified data processing pipeline"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#what-is-the-temperature-of-my-data","text":"Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"What is the temperature of my data?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/","text":"Collect Layer Types of data sources In Collection, you need to figure out the type of data you are collecting. Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NOSql database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed. Stream Storage For stream storage, we have 4 main options (1 open source & 3 in AWS) -> Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2. Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify th end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper. Which Streaming/Message Storage should I use? SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage. Which File/Object Storage should I use? Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data. S3 as Datalake storage One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. by having your storage outside of the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involves starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. this makes S3 durable. Security - it used encryption at rest and in transit both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section). Data Tiering The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3. What about Metadata? In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. AWS Glue Catalog It is a fully managed data catalog, and is Hive metastore compliant. Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify teh metadata for you so thaht you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions. Lakehouse Formation When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, fina matches where there is no matching id, etc. Hive Metastore If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS. Cache & Database Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache - DynamoDB Accelerator Graph DB Amazon Neptune Key-Value document Amazon DynamoDB SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard. Which Store should I use? Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Data Collection Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#collect-layer","text":"","title":"Collect Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#types-of-data-sources","text":"In Collection, you need to figure out the type of data you are collecting. Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NOSql database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed.","title":"Types of data sources"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#stream-storage","text":"For stream storage, we have 4 main options (1 open source & 3 in AWS) -> Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2. Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify th end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper.","title":"Stream Storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-streamingmessage-storage-should-i-use","text":"SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage.","title":"Which Streaming/Message Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-fileobject-storage-should-i-use","text":"Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data.","title":"Which File/Object Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#s3-as-datalake-storage","text":"One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. by having your storage outside of the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involves starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. this makes S3 durable. Security - it used encryption at rest and in transit both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section).","title":"S3 as Datalake storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#data-tiering","text":"The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3.","title":"Data Tiering"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#what-about-metadata","text":"In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. AWS Glue Catalog It is a fully managed data catalog, and is Hive metastore compliant. Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify teh metadata for you so thaht you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions. Lakehouse Formation When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, fina matches where there is no matching id, etc. Hive Metastore If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS.","title":"What about Metadata?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#cache-database","text":"Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache - DynamoDB Accelerator Graph DB Amazon Neptune Key-Value document Amazon DynamoDB SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard.","title":"Cache &amp; Database"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-store-should-i-use","text":"Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Which Store should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/","text":"Data Processing Layer We will highlight various available services in AWS and their use cases. Interactive & Batch Analytics First, let's look at interactive & batch analytics. Elastic Search - Elastic Search clusters can be spun up in minutes. - Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. - Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. - Redshift & Redshift Spectrum - for data warehousing needs. - Athena ~ Hive - perform SQL based queries on Data that resides on S3. - S3 Select - is somewhat similar to Athena, but there are some differences. - You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. - AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. - EMR - Its Hadoop & Spark as a service that lets you run various platform applications. - one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it. Streaming/Real-time Analytics If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data. In respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL. Predictive Analytics Application Services One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case. Platforms This layer allows us to build models and test them. Which Analytics should I use?","title":"Big data architectural patterns 4"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#data-processing-layer","text":"We will highlight various available services in AWS and their use cases.","title":"Data Processing Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#interactive-batch-analytics","text":"First, let's look at interactive & batch analytics. Elastic Search - Elastic Search clusters can be spun up in minutes. - Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. - Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. - Redshift & Redshift Spectrum - for data warehousing needs. - Athena ~ Hive - perform SQL based queries on Data that resides on S3. - S3 Select - is somewhat similar to Athena, but there are some differences. - You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. - AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. - EMR - Its Hadoop & Spark as a service that lets you run various platform applications. - one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it.","title":"Interactive &amp; Batch Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#streamingreal-time-analytics","text":"If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data. In respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL.","title":"Streaming/Real-time Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#predictive-analytics","text":"Application Services One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case. Platforms This layer allows us to build models and test them.","title":"Predictive Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#which-analytics-should-i-use","text":"","title":"Which Analytics should I use?"}]}