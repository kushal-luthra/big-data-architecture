{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Big Data Architecture Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture Topics Big data Architectural Patterns - Introduction - Big Data Architectural Principles - Data Collection Layer - Data Processing Layer Storage Layer - Row-based Vs Column-based File Formats - Text-based File Formats - Big Data File Formats","title":"Home"},{"location":"#welcome-to-big-data-architecture","text":"Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture","title":"Welcome to Big Data Architecture"},{"location":"#topics","text":"Big data Architectural Patterns - Introduction - Big Data Architectural Principles - Data Collection Layer - Data Processing Layer Storage Layer - Row-based Vs Column-based File Formats - Text-based File Formats - Big Data File Formats","title":"Topics"},{"location":"aboutme/","text":"About me Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData & Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos. He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks. Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.","title":"About Me"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/","text":"Big Data Analytics Architectural Patterns and Best Practices Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/#big-data-analytics-architectural-patterns-and-best-practices","text":"Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/","text":"There are different kind of analytical systems. Some customers are looking at doing batch-interactive analytics with their data. Others are looking at being able to to take real time data feeds, and store them for insights, and some go beyond, seking building of data models on top of it, power inferencing and do ML with that data. As we step step into the architecture discussion, we are going to discuss architecture for -> - Streaming processing - datalakes and batch interaction - machine learning. Note - you dont need to pick them right away. You can start small, architect it in a way that it enables you to build more and more features on top of it over time, incrementally. Now, lets look at different model of delivering big data services. Virtualized - eg- EC2 instances that we create & install kafka on top of it. Customer owns environment and manages it themselves. Managed Services - EMR (hadoop platform as a service), RDS - managed by AWS - customers still thinking about requirements like what configuration you need, what should be auto-scaling policy etc. Serverless/Clusterless/Containerized - Lambda, Athena, Glue - these are services that abstract out the servers away from you. This means users can focus on core use case instead of being caught up in acquiring cluster & configuring it, installing software etc. Various services - both open source and AWS based are mentioned below - Big Data Challenges Lets start with the challenge first. As a customer, one would like to have answers to questions like -> - Is there a reference architecture for my use case? - If yes, what tools should one choose? - How? - Why?","title":"Introduction"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/#big-data-challenges","text":"Lets start with the challenge first. As a customer, one would like to have answers to questions like -> - Is there a reference architecture for my use case? - If yes, what tools should one choose? - How? - Why?","title":"Big Data Challenges"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/","text":"Big Data Architectural Principles Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages. A simplified data processing pipeline Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders. What is the temperature of my data? Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#big-data-architectural-principles","text":"Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#a-simplified-data-processing-pipeline","text":"Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders.","title":"A simplified data processing pipeline"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#what-is-the-temperature-of-my-data","text":"Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"What is the temperature of my data?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/","text":"Collect Layer Types of data sources In Collection, you need to figure out the type of data you are collecting. Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NOSql database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed. Stream Storage For stream storage, we have 3 main options in AWS -> Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify th end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper. In open source we have Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2. Which Streaming/Message Storage should I use? SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage. Which File/Object Storage should I use? Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data. S3 as Datalake storage One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. by having your storage outside of the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involves starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. this makes S3 durable. Security - it used encryption at rest and in transit both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section). Data Tiering The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3. What about Metadata? In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. AWS Glue Catalog It is a fully managed data catalog, and is Hive metastore compliant. Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify teh metadata for you so thaht you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions. Lakehouse Formation When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, fina matches where there is no matching id, etc. Hive Metastore If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS. Cache & Database Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache - DynamoDB Accelerator Graph DB Amazon Neptune Key-Value document Amazon DynamoDB SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard. Which Store should I use? Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Data Collection Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#collect-layer","text":"","title":"Collect Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#types-of-data-sources","text":"In Collection, you need to figure out the type of data you are collecting. Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NOSql database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed.","title":"Types of data sources"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#stream-storage","text":"For stream storage, we have 3 main options in AWS -> Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify th end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper. In open source we have Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2.","title":"Stream Storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-streamingmessage-storage-should-i-use","text":"SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage.","title":"Which Streaming/Message Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-fileobject-storage-should-i-use","text":"Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data.","title":"Which File/Object Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#s3-as-datalake-storage","text":"One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. by having your storage outside of the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involves starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. this makes S3 durable. Security - it used encryption at rest and in transit both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section).","title":"S3 as Datalake storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#data-tiering","text":"The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3.","title":"Data Tiering"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#what-about-metadata","text":"In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. AWS Glue Catalog It is a fully managed data catalog, and is Hive metastore compliant. Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify teh metadata for you so thaht you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions. Lakehouse Formation When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, fina matches where there is no matching id, etc. Hive Metastore If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS.","title":"What about Metadata?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#cache-database","text":"Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache - DynamoDB Accelerator Graph DB Amazon Neptune Key-Value document Amazon DynamoDB SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard.","title":"Cache &amp; Database"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-store-should-i-use","text":"Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Which Store should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/","text":"Data Processing Layer We will highlight various available services in AWS and their use cases. Interactive & Batch Analytics First, let's look at interactive & batch analytics. Elastic Search - Elastic Search clusters can be spun up in minutes. - Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. - Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. - Redshift & Redshift Spectrum - for data warehousing needs. - Athena ~ Hive - perform SQL based queries on Data that resides on S3. - S3 Select - is somewhat similar to Athena, but there are some differences. - You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. - AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. - EMR - Its Hadoop & Spark as a service that lets you run various platform applications. - one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it. Streaming/Real-time Analytics If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data. In respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL. Predictive Analytics Application Services One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case. Platforms This layer allows us to build models and test them. Which Analytics should I use?","title":"Data Processing Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#data-processing-layer","text":"We will highlight various available services in AWS and their use cases.","title":"Data Processing Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#interactive-batch-analytics","text":"First, let's look at interactive & batch analytics. Elastic Search - Elastic Search clusters can be spun up in minutes. - Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. - Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. - Redshift & Redshift Spectrum - for data warehousing needs. - Athena ~ Hive - perform SQL based queries on Data that resides on S3. - S3 Select - is somewhat similar to Athena, but there are some differences. - You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. - AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. - EMR - Its Hadoop & Spark as a service that lets you run various platform applications. - one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it.","title":"Interactive &amp; Batch Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#streamingreal-time-analytics","text":"If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data. In respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL.","title":"Streaming/Real-time Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#predictive-analytics","text":"Application Services One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case. Platforms This layer allows us to build models and test them.","title":"Predictive Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#which-analytics-should-i-use","text":"","title":"Which Analytics should I use?"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/","text":"Text File format Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text-based File Formats"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/#text-file-format","text":"Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text File format"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/","text":"Avro, ORC and Parquet There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Big Data File Formats"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/#avro-orc-and-parquet","text":"There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Avro, ORC and Parquet"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/","text":"Row Vs Column File Format When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row-based Vs Column-based File Formats"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/#row-vs-column-file-format","text":"When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row Vs Column File Format"},{"location":"Storage%20Layer%20Choicesold/Text%20File%20Formats/","text":"Text File format Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2013 \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text File Formats"},{"location":"Storage%20Layer%20Choicesold/big%20data%20file%20formats/","text":"Avro, ORC and Parquet There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. \uf0a7 KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Big data file formats"},{"location":"Storage%20Layer%20Choicesold/big%20data%20file%20formats/#avro-orc-and-parquet","text":"There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. \uf0a7 KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Avro, ORC and Parquet"},{"location":"Storage%20Layer%20Choicesold/row%20based%20and%20column%20based%20file%20formats/","text":"Row Vs Column File Format When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row based and column based file formats"},{"location":"Storage%20Layer%20Choicesold/row%20based%20and%20column%20based%20file%20formats/#row-vs-column-file-format","text":"When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row Vs Column File Format"}]}