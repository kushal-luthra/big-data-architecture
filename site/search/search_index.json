{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Big Data Architecture Topics Big data Architectural Patterns - Introduction - Big Data Architectural Principles - Data Collection Layer - Data Processing Layer - Data Consumption Layer - Putting it all together - Design Patterns Storage Layer - Row-based Vs Column-based File Formats - Text-based File Formats - Big Data File Formats - File Compression Techniques in Big Data Systems HBase - Why HBase? - HBase Properties Case Study - IHS Markit System Design Case Study (2020) - ZS Associates Data Consultant System Design Case Study (2020) - Interview Questions Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture","title":"Home"},{"location":"#welcome-to-big-data-architecture","text":"","title":"Welcome to Big Data Architecture"},{"location":"#topics","text":"Big data Architectural Patterns - Introduction - Big Data Architectural Principles - Data Collection Layer - Data Processing Layer - Data Consumption Layer - Putting it all together - Design Patterns Storage Layer - Row-based Vs Column-based File Formats - Text-based File Formats - Big Data File Formats - File Compression Techniques in Big Data Systems HBase - Why HBase? - HBase Properties Case Study - IHS Markit System Design Case Study (2020) - ZS Associates Data Consultant System Design Case Study (2020) - Interview Questions Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture","title":"Topics"},{"location":"aboutme/","text":"Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData & Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos. Efficient team leader and mentor with a history of managing multiple squads. Expert in providing data driven strategic approach and creating reusable and scalable models. Building strategic assets to set up a long term sustainable operational and capability model in technology strategy arena. He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks. Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.","title":"About Me"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/","text":"Big Data Analytics Architectural Patterns and Best Practices Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/#big-data-analytics-architectural-patterns-and-best-practices","text":"Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/","text":"There are different kind of analytical systems. Some customers are looking at doing batch-interactive analytics with their data. Others are looking at being able to to take real time data feeds, and store them for insights, and some go beyond, seking building of data models on top of it, power inferencing and do ML with that data. As we step step into the architecture discussion, we are going to discuss architecture for -> - Streaming processing - datalakes and batch interaction - machine learning. Note - you dont need to pick them right away. You can start small, architect it in a way that it enables you to build more and more features on top of it over time, incrementally. Now, lets look at different model of delivering big data services. Virtualized - eg- EC2 instances that we create & install kafka on top of it. Customer owns environment and manages it themselves. Managed Services - EMR (hadoop platform as a service), RDS - managed by AWS - customers still thinking about requirements like what configuration you need, what should be auto-scaling policy etc. Serverless/Clusterless/Containerized - Lambda, Athena, Glue - these are services that abstract out the servers away from you. This means users can focus on core use case instead of being caught up in acquiring cluster & configuring it, installing software etc. Various services - both open source and AWS based are mentioned below - Big Data Challenges Lets start with the challenge first. As a customer, one would like to have answers to questions like -> - Is there a reference architecture for my use case? - If yes, what tools should one choose? - How? - Why?","title":"Introduction"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/#big-data-challenges","text":"Lets start with the challenge first. As a customer, one would like to have answers to questions like -> - Is there a reference architecture for my use case? - If yes, what tools should one choose? - How? - Why?","title":"Big Data Challenges"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/","text":"Big Data Architectural Principles Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages. A simplified data processing pipeline Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders. What is the temperature of my data? Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#big-data-architectural-principles","text":"Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#a-simplified-data-processing-pipeline","text":"Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders.","title":"A simplified data processing pipeline"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#what-is-the-temperature-of-my-data","text":"Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"What is the temperature of my data?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/","text":"Collect Layer Types of data sources In Collection, you need to figure out the type of data you are collecting . Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NoSQL database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. Transactional data For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). Files/Objects data For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed. Stream Storage AWS-based Stream Storage options For stream storage, we have 3 main options in AWS -> Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify the end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper. Apache Kafka : Stream Storage In open source we have Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2. Which Streaming/Message Storage should I use? SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage. Which File/Object Storage should I use? Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data. S3 as Datalake storage One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. By having your storage outside the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involve starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. This makes S3 durable. Security - it used encryption at rest and in transit, both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section). Data Tiering & S3 cost optimization strategy The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3. What about Metadata? In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. We have 3 options -> AWS Glue Catalog It is a fully managed data catalog , and is Hive metastore compliant . Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify the metadata for you so that you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions. Lakehouse Formation When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, find matches where there is no matching id, etc. Hive Metastore If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS. Cache & Database Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache, Redis - DynamoDB Accelerator Graph DB Amazon Neptune , neo4j Key-Value document Amazon DynamoDB , Cassandra SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard. Which database store should I use? Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Data Collection Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#collect-layer","text":"","title":"Collect Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#types-of-data-sources","text":"In Collection, you need to figure out the type of data you are collecting . Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NoSQL database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different.","title":"Types of data sources"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#transactional-data","text":"For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this).","title":"Transactional data"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#filesobjects-data","text":"For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed.","title":"Files/Objects data"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#stream-storage","text":"","title":"Stream Storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#aws-based-stream-storage-options","text":"For stream storage, we have 3 main options in AWS -> Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify the end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper.","title":"AWS-based Stream Storage options"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#apache-kafka-stream-storage","text":"In open source we have Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2.","title":"Apache Kafka : Stream Storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-streamingmessage-storage-should-i-use","text":"SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage.","title":"Which Streaming/Message Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-fileobject-storage-should-i-use","text":"Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data.","title":"Which File/Object Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#s3-as-datalake-storage","text":"One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. By having your storage outside the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involve starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. This makes S3 durable. Security - it used encryption at rest and in transit, both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section).","title":"S3 as Datalake storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#data-tiering-s3-cost-optimization-strategy","text":"The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3.","title":"Data Tiering &amp; S3 cost optimization strategy"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#what-about-metadata","text":"In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. We have 3 options ->","title":"What about Metadata?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#aws-glue-catalog","text":"It is a fully managed data catalog , and is Hive metastore compliant . Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify the metadata for you so that you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions.","title":"AWS Glue Catalog"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#lakehouse-formation","text":"When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, find matches where there is no matching id, etc.","title":"Lakehouse Formation"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#hive-metastore","text":"If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS.","title":"Hive Metastore"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#cache-database","text":"Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache, Redis - DynamoDB Accelerator Graph DB Amazon Neptune , neo4j Key-Value document Amazon DynamoDB , Cassandra SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard.","title":"Cache &amp; Database"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-database-store-should-i-use","text":"Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Which database store should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/","text":"Data Processing Layer We will highlight various available services in AWS and their use cases. Interactive & Batch Analytics First, let's look at interactive & batch analytics. Amazon Elastic Search Service Ir is a managed service for Elastic Search. Elastic Search clusters can be spun up in minutes. Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. Amazon Redshift & Redshift Spectrum for data warehousing needs. Redhsift is a fully managed Data Warehouse. AWS Redshift is an OLAP system. Its an MPP (Massively Parallel Processing) System . It will allow us to query terabytes of data. Its a schema on read kind of system, wherein you define Schema first and put in a highly structured data. Redshift Spectrum enables querying S3. Amazon Athena ~ Hive It is Serverless interactive query service. Performs SQL based queries on Data that resides on S3. Similar to Presto/Hive. S3 Select is somewhat similar to Athena, but there are some differences. You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. Athena works on logical object selecting its information from AWS Glue Catalog rather than selecting data out of an object on S3. S3 Select works on object out of S3 bucket. Amazon EMR Its Hadoop & Spark as a Service that lets you run various platform applications. one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it. Streaming/Real-time Analytics In Streaming analytics we have a number of options. 1. Spark Streaming on Amazon EMR. 2. Amazon Kinesis Data Analytics - 1. Its ia managed service for running SQL on streaming data. 3. Amazon KCL 1. You can always create your own application by using Amazon Kinesis Client Library. 4. Amazon Lambda 1. Run code serverless (without provisioning or managing servers). 2. Services such as S3 can publish events to AWS Lambda. 3. AWS Lambda can pool events from a Kinesis. If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data, and under the hood it will be working on real-time data feeds, and sending back the output. As a customer one key question that needs to be answered while picking a tool is - 'What is my end to end latency from point at which I publish a message to point at which I process the message and get final results?' AWS Lambda lets you write functions, and set up an event trigger for Kinesis off of that. But, in respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL. Predictive Analytics Here you have the option to have 3 layers - 1. Frameworks 2. Platforms 3. Application Services Frameworks It is for ML Practitioners, wherein you get the bare machines with those frameworks pre-installed, and you have to do traditional Machine Learning tasks like hyper-parameter tuning, etc. Platforms This layer allows us to build models and test them. Helps ML experts to dod tasks like giving them jupyter notebooks, familiar interfacce, and then you can use Sagemaker API to create instance models and then you can deploy the model. Its one kind of place where you can do all ML tasks on the platform. Application Services It is for developers who want to use ML in their aplication, but dont want to implement ML So, we have some algorithm which are successful like NLP, and these can be integrated by calling these APIs. Application services help built different types of ML algos on the platform. One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case. Which Analytics should I use? Batch analytics - think of it as a report that runs daily/weekly/monthly. Tools - Amazon EMR, Hive/Spark. Interactive Analytics See it is a s self-service dashboard, wherein you want to get answers yourself from the system. Tools - Redshift, Athena, Amzon EMR on Spark/Presto. Stream Analytics In EMR, you can utilize Spark Streaming, Kinesis Data analytics services. Predictive Analytics It can be batch or streaming. ETL Often times data comes in raw form, and what you want to do is to create curated datasets or canonical datasets, that represent a normalized view of data. And that's where ETL process comes in. And that normalized dataset is often in format different from the one that came in. So, your raw data could be in JSON, CSV etc, and your output would often be a column oriented format(parquet/orc), or row-based format(Avro - handles schema changes better). One concern around ETL is that one loses value of data. But in truth, ETL helps transform data in format that is more suitable for doing analytics - partitioning, converting form text to parquert/orc. Below are AWS ETL options. AWS Glue ETL Allows running spark/python shell. it is a serverless category wherein you abstract server complexity from user. you define data processing unit and schedule those jobs to run. Amazon Data Migration Service(DMS) If you want to have CDC (Change Data Capture), then Amazon Data Migration Service(DMS) is an option. Glue isn't abl;e to do that, or you can look at CDC solution from partner services.","title":"Data Processing Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#data-processing-layer","text":"We will highlight various available services in AWS and their use cases.","title":"Data Processing Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#interactive-batch-analytics","text":"First, let's look at interactive & batch analytics. Amazon Elastic Search Service Ir is a managed service for Elastic Search. Elastic Search clusters can be spun up in minutes. Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. Amazon Redshift & Redshift Spectrum for data warehousing needs. Redhsift is a fully managed Data Warehouse. AWS Redshift is an OLAP system. Its an MPP (Massively Parallel Processing) System . It will allow us to query terabytes of data. Its a schema on read kind of system, wherein you define Schema first and put in a highly structured data. Redshift Spectrum enables querying S3. Amazon Athena ~ Hive It is Serverless interactive query service. Performs SQL based queries on Data that resides on S3. Similar to Presto/Hive. S3 Select is somewhat similar to Athena, but there are some differences. You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. Athena works on logical object selecting its information from AWS Glue Catalog rather than selecting data out of an object on S3. S3 Select works on object out of S3 bucket. Amazon EMR Its Hadoop & Spark as a Service that lets you run various platform applications. one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it.","title":"Interactive &amp; Batch Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#streamingreal-time-analytics","text":"In Streaming analytics we have a number of options. 1. Spark Streaming on Amazon EMR. 2. Amazon Kinesis Data Analytics - 1. Its ia managed service for running SQL on streaming data. 3. Amazon KCL 1. You can always create your own application by using Amazon Kinesis Client Library. 4. Amazon Lambda 1. Run code serverless (without provisioning or managing servers). 2. Services such as S3 can publish events to AWS Lambda. 3. AWS Lambda can pool events from a Kinesis. If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data, and under the hood it will be working on real-time data feeds, and sending back the output. As a customer one key question that needs to be answered while picking a tool is - 'What is my end to end latency from point at which I publish a message to point at which I process the message and get final results?' AWS Lambda lets you write functions, and set up an event trigger for Kinesis off of that. But, in respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL.","title":"Streaming/Real-time Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#predictive-analytics","text":"Here you have the option to have 3 layers - 1. Frameworks 2. Platforms 3. Application Services Frameworks It is for ML Practitioners, wherein you get the bare machines with those frameworks pre-installed, and you have to do traditional Machine Learning tasks like hyper-parameter tuning, etc. Platforms This layer allows us to build models and test them. Helps ML experts to dod tasks like giving them jupyter notebooks, familiar interfacce, and then you can use Sagemaker API to create instance models and then you can deploy the model. Its one kind of place where you can do all ML tasks on the platform. Application Services It is for developers who want to use ML in their aplication, but dont want to implement ML So, we have some algorithm which are successful like NLP, and these can be integrated by calling these APIs. Application services help built different types of ML algos on the platform. One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case.","title":"Predictive Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#which-analytics-should-i-use","text":"Batch analytics - think of it as a report that runs daily/weekly/monthly. Tools - Amazon EMR, Hive/Spark. Interactive Analytics See it is a s self-service dashboard, wherein you want to get answers yourself from the system. Tools - Redshift, Athena, Amzon EMR on Spark/Presto. Stream Analytics In EMR, you can utilize Spark Streaming, Kinesis Data analytics services. Predictive Analytics It can be batch or streaming.","title":"Which Analytics should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#etl","text":"Often times data comes in raw form, and what you want to do is to create curated datasets or canonical datasets, that represent a normalized view of data. And that's where ETL process comes in. And that normalized dataset is often in format different from the one that came in. So, your raw data could be in JSON, CSV etc, and your output would often be a column oriented format(parquet/orc), or row-based format(Avro - handles schema changes better). One concern around ETL is that one loses value of data. But in truth, ETL helps transform data in format that is more suitable for doing analytics - partitioning, converting form text to parquert/orc. Below are AWS ETL options.","title":"ETL"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#aws-glue-etl","text":"Allows running spark/python shell. it is a serverless category wherein you abstract server complexity from user. you define data processing unit and schedule those jobs to run.","title":"AWS Glue ETL"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#amazon-data-migration-servicedms","text":"If you want to have CDC (Change Data Capture), then Amazon Data Migration Service(DMS) is an option. Glue isn't abl;e to do that, or you can look at CDC solution from partner services.","title":"Amazon Data Migration Service(DMS)"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-5/","text":"Data Consumption Process When it comes to consumption, we have 2 categories of users - 1. Business users - who want to make sense of data. The service here involved is applications like visualization applications like Tableau and Amazon Quicksight (amazon managed visualization tool), Kibana (visualization on elastic search). Data Scientist - They want to get access to an endpoint and play with data. They would like to use Athena, Redshift, etc. We could ask questions like what type of BI or UI one could use in this type of solution. Answer depends on who is user and what is the function they are doing. Eg - Business user would not prefer to be asked to use Jupyter notebooks, nor would a data scientist enjoy being placed in front of dashboard with little flexibility to play around with data. In this stage, solutions you would find would be a suite of different UI being used to be able to perform these analytics. Jupyter - used in data science space. Sagemaker allows you to have a managed Jupyter environment, running both Jupyter and Jupyter Lab. You could also run managed Jupyter or managed notebooks on EMR. So, if you are looking at just Hadoop ecosystem, we have option of EMR or managed hadoop environment. ELK Stack - Kibana can allow doing Log analysis. There are also partner products like Splunk, Tableau, Looker and MircoStrategy.","title":"Data Consumption Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-5/#data-consumption-process","text":"When it comes to consumption, we have 2 categories of users - 1. Business users - who want to make sense of data. The service here involved is applications like visualization applications like Tableau and Amazon Quicksight (amazon managed visualization tool), Kibana (visualization on elastic search). Data Scientist - They want to get access to an endpoint and play with data. They would like to use Athena, Redshift, etc. We could ask questions like what type of BI or UI one could use in this type of solution. Answer depends on who is user and what is the function they are doing. Eg - Business user would not prefer to be asked to use Jupyter notebooks, nor would a data scientist enjoy being placed in front of dashboard with little flexibility to play around with data. In this stage, solutions you would find would be a suite of different UI being used to be able to perform these analytics. Jupyter - used in data science space. Sagemaker allows you to have a managed Jupyter environment, running both Jupyter and Jupyter Lab. You could also run managed Jupyter or managed notebooks on EMR. So, if you are looking at just Hadoop ecosystem, we have option of EMR or managed hadoop environment. ELK Stack - Kibana can allow doing Log analysis. There are also partner products like Splunk, Tableau, Looker and MircoStrategy.","title":"Data Consumption Process"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-6/","text":"Putting it all together We have data from various sources flowing in. Different repositories based on requirements that are needed to store that data. One key call-outs we need to take note of is ETL process - How can I take my data, create it in the most consumable method for different types of users to be able to process and analyze that data, and ultimately getting it to derive insights and answer business questions, etc.","title":"Putting it all together"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-6/#putting-it-all-together","text":"We have data from various sources flowing in. Different repositories based on requirements that are needed to store that data. One key call-outs we need to take note of is ETL process - How can I take my data, create it in the most consumable method for different types of users to be able to process and analyze that data, and ultimately getting it to derive insights and answer business questions, etc.","title":"Putting it all together"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/","text":"Design Patterns What is the temperature of my Data? As we talk about design patterns, it takes us back to our original question - 'What is the temperature of my Data?' Moving from left-hand side, we have Kinesis, Databases, Transactional databases and all the way to Data lakes & Cold storages. On LHS we see Hot data , which has the following properties - - data that is short-lived. - Such data loses value as it ages. - data you want to retrieve faster. On RHS, we have Cold Data , which has the following properties - - Data that consists of old records, which are archived. For Hot Data , you typically ingest using Amazon Kinesis, and perform Streaming Analytics using Spark Streaming/AWS Lambda/KCL Apps. For Interactive Analytics , it is done using multiple data sources - streaming/batch. Here typical practise w.r.t streaming data based interactive analytics is to useKinesis stream, pipe it to S3, and use suite of tools available to you to be able to process that data once it hits S3. You use Amazon Redshift/Athena/Spark-Presto combination. For Batch Analytics , where response time is highest, you can use Hive or AWS Batch technologies. Streaming Analytics For Streaming data, very often you would want to do real-time analytics on that data, for which you could use Amazon Kinesis Data Analytics . Other options could be - - Amazon Lambda - to read your Stream. - KCL App - Micro batching - using spark streaming on Amazon EMR. Further, as you process the stream, you could do real time predictive analytics on your data (i.e. ML) by utilizing end-points that are already available as part of Amazon AI ecosystem like Sagemaker , which allows you to build these models, and have an end point that these services could call. Likewise, you could do Fraud Alerts and send notification to users using Amazon SNS . You can store data to S3 for * doing analytics later on. If you want to also capture the data to have real-time dashboard or feed a dashboard, then it is a good practise to export like an App State or Materialized View and have another system that could be a database, like DynamoDb kind of system, and KPI dashboard being setup in front of streaming data. App Store is based on cache , including Memcache, Redis or Amazon ElasticCache. Example - Hearst's Serverless Data Pipeline Hearst is a media and information company, and they have wide number of different channels - magazine, newspapers, media channels like ESPN, etc. under their umbrella. And so they have various kinds of information coming from different websites, and going into Amazon Kinesis Data Streams . Once it hits the data streams, and since it is a stream, so you could have multiple consumers - - one of those consumers coud store my data back into a datalake or data repository. - another consumer could build real-time dashboard or build a real-time analysis of that data. This is what they are doing. Their data is flowing to Firehose which is used to capture Streaming events and delivering them into S3 for further analytics later on. Another consumer is going through lambda pipeline, pulling data from the stream, doing analytics and push the results of analysis into DynamoDb. - In DynamoDb, we store that app state or store that analysis state which is exposed through an API Gateway, that allows you to have a REST-Ful endpoint, defined through swagger to get the current state of data. Example 2 - Yieldmo's data ingestion and real-time analytics architecture They have 1000s of mobile devices whose data is passed through kinesis data streams. On it, they are running kinesis data analytics. In Hearst, they used lambda - wherein their developers wrote function codes to be able to process the real-time data in order to store it into DynamoDb tables. In Yieldmo, customer is writing SQL to be able to run same sort of processing on real-time feeds, and then do aggregation and filtering on that data, and store it into data warehouse using Firehose and S3 channel. Interactive & Batch Analytics In above diagram, Top layer is Interactive Analytics and Bottom layer is Batch analytics. Interactive & Batch pipeline will have data with different latencies - - Streaming data - The fact that you might do interactive and batch analytics doesn't mean you don't have streaming data. So you have streaming data, and you use Kinesis Firehose to capture them into S3. For example, if you have ClickStream data, you might want to start analyzing data interactively like What certain users are doing, what different profile of users I have? As soon as Streaming data flows in, it gets pushed into S3, and simultaneously gets loaded into Redshift for data warehousing or Elastic Search to do Kibana dash-boarding based analysis. For processing such data over S3, you could use Athena or EMR with Spark/Presto. - Batch data - you also might have files that you would want to deliver directly to S3. This data would be coming from different data sources coming into S3 through wide variety of methods - Snowball, S3 accelerate transfer, integrating data from different departments in the company, etc. For Batch analytics, here 2 main tools are - EMR and Glue jobs. For Batch processing in EMR, we earlier used Hive and Pig, and now Spark is being used. Example - FINRA : Migrating to AWS FINRA is the financial regulatory authority of USA. It ingests 75 billion events per day, which this includes - Stock information, Financial Trade information, and what they are really looking is find things like Market Manipulators, and other anomalous behaviour on the stock market. Data is coming from various sources into S3. - All this data flows into S3. - They have canonical data issue - different brokerage report data in different formats. It goes in the raw form into canonical form and using wide number of EMR & redshift clusters to process data. From S3, they have multiple systems doing analytics, and this is back to where we say, you can have multiple clusters doing analytics, and these clusters would be specific to workload you want to support. For example - - redshift - pre-defined queries. - multiple EMR clusters to perform interactive analytics and Surveillance analytics for detecting fraud. This shows the flexibility in your architecture - wherein you are storing data in format which is easily consumable by - - multiple types or same service like EMR - data warehouse - data lake analytics by your ML. FINRA has strict security requirements , and are utilizing things like VPC. Redshift and EMR can be hosted into VPC. They also use encryption at rest and in transit in S3. They are also using Cloud Trail , which is for audit purposes. Cloud Trail captures every single API call into AWS, and you can have an audit trail for any action that happens on your infrastructure. Datalake Final pattern in our reference architecture is 'Datalake'. Here Amazon S3, along with Glue Catalog, together are playing central role. - When you are building a data lake, its about tying in interactive and batch processing, wherein S3 is centre of lake. - Another important aspect of datalake is metadata management -AWS Glue Catalog, which serves as common metadata store. There are multiple ways to put data into S3. - If you have Relational Databases, and you want to have CDC, Database Migration Service is a great way. Other way is with Glue ETL jobs top put data into S3. - For Streaming Data arriving, Amazon Firehose is a great way - you can use Kinesis data analytics to do data analytics and them put them back to Stream, and then put them back to S3. - From Amazon S3, you can use lambda or spark streaming or KCL applications to do a real-time analytics, and then load data into some sort of application state or materialized view so that you drive dashboard or you can have batch or interactive layer above your data lake so that you can explore data in data lake. Service as for batch and interactive analytics here are - Redshift, Athena, EMR, Presto.","title":"Design Patterns"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#design-patterns","text":"","title":"Design Patterns"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#what-is-the-temperature-of-my-data","text":"As we talk about design patterns, it takes us back to our original question - 'What is the temperature of my Data?' Moving from left-hand side, we have Kinesis, Databases, Transactional databases and all the way to Data lakes & Cold storages. On LHS we see Hot data , which has the following properties - - data that is short-lived. - Such data loses value as it ages. - data you want to retrieve faster. On RHS, we have Cold Data , which has the following properties - - Data that consists of old records, which are archived. For Hot Data , you typically ingest using Amazon Kinesis, and perform Streaming Analytics using Spark Streaming/AWS Lambda/KCL Apps. For Interactive Analytics , it is done using multiple data sources - streaming/batch. Here typical practise w.r.t streaming data based interactive analytics is to useKinesis stream, pipe it to S3, and use suite of tools available to you to be able to process that data once it hits S3. You use Amazon Redshift/Athena/Spark-Presto combination. For Batch Analytics , where response time is highest, you can use Hive or AWS Batch technologies.","title":"What is the temperature of my Data?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#streaming-analytics","text":"For Streaming data, very often you would want to do real-time analytics on that data, for which you could use Amazon Kinesis Data Analytics . Other options could be - - Amazon Lambda - to read your Stream. - KCL App - Micro batching - using spark streaming on Amazon EMR. Further, as you process the stream, you could do real time predictive analytics on your data (i.e. ML) by utilizing end-points that are already available as part of Amazon AI ecosystem like Sagemaker , which allows you to build these models, and have an end point that these services could call. Likewise, you could do Fraud Alerts and send notification to users using Amazon SNS . You can store data to S3 for * doing analytics later on. If you want to also capture the data to have real-time dashboard or feed a dashboard, then it is a good practise to export like an App State or Materialized View and have another system that could be a database, like DynamoDb kind of system, and KPI dashboard being setup in front of streaming data. App Store is based on cache , including Memcache, Redis or Amazon ElasticCache.","title":"Streaming Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#example-hearsts-serverless-data-pipeline","text":"Hearst is a media and information company, and they have wide number of different channels - magazine, newspapers, media channels like ESPN, etc. under their umbrella. And so they have various kinds of information coming from different websites, and going into Amazon Kinesis Data Streams . Once it hits the data streams, and since it is a stream, so you could have multiple consumers - - one of those consumers coud store my data back into a datalake or data repository. - another consumer could build real-time dashboard or build a real-time analysis of that data. This is what they are doing. Their data is flowing to Firehose which is used to capture Streaming events and delivering them into S3 for further analytics later on. Another consumer is going through lambda pipeline, pulling data from the stream, doing analytics and push the results of analysis into DynamoDb. - In DynamoDb, we store that app state or store that analysis state which is exposed through an API Gateway, that allows you to have a REST-Ful endpoint, defined through swagger to get the current state of data.","title":"Example - Hearst's Serverless Data Pipeline"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#example-2-yieldmos-data-ingestion-and-real-time-analytics-architecture","text":"They have 1000s of mobile devices whose data is passed through kinesis data streams. On it, they are running kinesis data analytics. In Hearst, they used lambda - wherein their developers wrote function codes to be able to process the real-time data in order to store it into DynamoDb tables. In Yieldmo, customer is writing SQL to be able to run same sort of processing on real-time feeds, and then do aggregation and filtering on that data, and store it into data warehouse using Firehose and S3 channel.","title":"Example 2 - Yieldmo's data ingestion and real-time analytics architecture"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#interactive-batch-analytics","text":"In above diagram, Top layer is Interactive Analytics and Bottom layer is Batch analytics. Interactive & Batch pipeline will have data with different latencies - - Streaming data - The fact that you might do interactive and batch analytics doesn't mean you don't have streaming data. So you have streaming data, and you use Kinesis Firehose to capture them into S3. For example, if you have ClickStream data, you might want to start analyzing data interactively like What certain users are doing, what different profile of users I have? As soon as Streaming data flows in, it gets pushed into S3, and simultaneously gets loaded into Redshift for data warehousing or Elastic Search to do Kibana dash-boarding based analysis. For processing such data over S3, you could use Athena or EMR with Spark/Presto. - Batch data - you also might have files that you would want to deliver directly to S3. This data would be coming from different data sources coming into S3 through wide variety of methods - Snowball, S3 accelerate transfer, integrating data from different departments in the company, etc. For Batch analytics, here 2 main tools are - EMR and Glue jobs. For Batch processing in EMR, we earlier used Hive and Pig, and now Spark is being used.","title":"Interactive &amp; Batch Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#example-finra-migrating-to-aws","text":"FINRA is the financial regulatory authority of USA. It ingests 75 billion events per day, which this includes - Stock information, Financial Trade information, and what they are really looking is find things like Market Manipulators, and other anomalous behaviour on the stock market. Data is coming from various sources into S3. - All this data flows into S3. - They have canonical data issue - different brokerage report data in different formats. It goes in the raw form into canonical form and using wide number of EMR & redshift clusters to process data. From S3, they have multiple systems doing analytics, and this is back to where we say, you can have multiple clusters doing analytics, and these clusters would be specific to workload you want to support. For example - - redshift - pre-defined queries. - multiple EMR clusters to perform interactive analytics and Surveillance analytics for detecting fraud. This shows the flexibility in your architecture - wherein you are storing data in format which is easily consumable by - - multiple types or same service like EMR - data warehouse - data lake analytics by your ML. FINRA has strict security requirements , and are utilizing things like VPC. Redshift and EMR can be hosted into VPC. They also use encryption at rest and in transit in S3. They are also using Cloud Trail , which is for audit purposes. Cloud Trail captures every single API call into AWS, and you can have an audit trail for any action that happens on your infrastructure.","title":"Example -  FINRA : Migrating to AWS"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-7/#datalake","text":"Final pattern in our reference architecture is 'Datalake'. Here Amazon S3, along with Glue Catalog, together are playing central role. - When you are building a data lake, its about tying in interactive and batch processing, wherein S3 is centre of lake. - Another important aspect of datalake is metadata management -AWS Glue Catalog, which serves as common metadata store. There are multiple ways to put data into S3. - If you have Relational Databases, and you want to have CDC, Database Migration Service is a great way. Other way is with Glue ETL jobs top put data into S3. - For Streaming Data arriving, Amazon Firehose is a great way - you can use Kinesis data analytics to do data analytics and them put them back to Stream, and then put them back to S3. - From Amazon S3, you can use lambda or spark streaming or KCL applications to do a real-time analytics, and then load data into some sort of application state or materialized view so that you drive dashboard or you can have batch or interactive layer above your data lake so that you can explore data in data lake. Service as for batch and interactive analytics here are - Redshift, Athena, EMR, Presto.","title":"Datalake"},{"location":"Data%20Platform%20Architecture/1/","text":"","title":"1"},{"location":"HBase/1-HBase/","text":"4 conditions for a database Structured data \u2013 data is stored in rows and columns Random access \u2013 to values Low latency \u2013 in a given set of rows if you want to search one row, it can be done in short span of time. We have features like indexes to facilitate this. Both low latency and random access are co-related \u2013 both happen alongside. ACID properties. ACID Properties Atomic Eg \u2013 A is transferring Rs 5000 to B\u2019s a/c, then 2 operations are happening \u2013 debit in A\u2019s a/c and credit in B\u2019s a/c Atomicity means either both operations happen, or none happen. Consistency It means the constraints should be met. Eg- we have employee table, and I want each employee to have new ID, here we can place unique key constraint. We can place different kinds of constraints to ensure data is consistent as per our requirements. Isolation Isolation means that If 2 persons are operating on a row \u2013 both are making a change to row, then these operations shouldn\u2019t happen at random. There should be a sequence associated with them through lock mechanism. That is concurrent operations on database should appear as though they happen in some sequence. Durability It means that whenever the system fails due crash, power cut etc, it should not be that system comes down. That is data is safe. Things should be up and running after failure. Hadoop makes a very poor database, because ... Supports unstructured data No random access Data kept in form of files and not rows and columns. So, random access not possible. High latency No ACID compliance To overcome limitation of random access or quick access of data, google published a paper on Big Table , a distributed storage system for structured data. To recall, google gave 2 main papers, initially \u2013 \u2022 GFS or google file system \u2013 for storage \u2022 MapReduce \u2013 for processing Now, it gave another paper for quick searching \u2013 BigTable. Hbase is a distributed database management system, that runs on top of Hadoop. So, if you have 4 node Hadoop cluster, Hbase runs on this Hadoop cluster. Hbase properties \u2022 Distributed \u2013 stores data in HDFS \u2022 Scalable \u2013 capacity directly proportional to number of nodes in the cluster. \u2022 Fault tolerant \u2013 piggybacks on Hadoop. HDFS gives replication of data, and so same for HBase.","title":"Why HBase"},{"location":"HBase/1-HBase/#4-conditions-for-a-database","text":"Structured data \u2013 data is stored in rows and columns Random access \u2013 to values Low latency \u2013 in a given set of rows if you want to search one row, it can be done in short span of time. We have features like indexes to facilitate this. Both low latency and random access are co-related \u2013 both happen alongside. ACID properties.","title":"4 conditions for a database"},{"location":"HBase/1-HBase/#acid-properties","text":"Atomic Eg \u2013 A is transferring Rs 5000 to B\u2019s a/c, then 2 operations are happening \u2013 debit in A\u2019s a/c and credit in B\u2019s a/c Atomicity means either both operations happen, or none happen. Consistency It means the constraints should be met. Eg- we have employee table, and I want each employee to have new ID, here we can place unique key constraint. We can place different kinds of constraints to ensure data is consistent as per our requirements. Isolation Isolation means that If 2 persons are operating on a row \u2013 both are making a change to row, then these operations shouldn\u2019t happen at random. There should be a sequence associated with them through lock mechanism. That is concurrent operations on database should appear as though they happen in some sequence. Durability It means that whenever the system fails due crash, power cut etc, it should not be that system comes down. That is data is safe. Things should be up and running after failure.","title":"ACID Properties"},{"location":"HBase/1-HBase/#hadoop-makes-a-very-poor-database-because","text":"Supports unstructured data No random access Data kept in form of files and not rows and columns. So, random access not possible. High latency No ACID compliance To overcome limitation of random access or quick access of data, google published a paper on Big Table , a distributed storage system for structured data. To recall, google gave 2 main papers, initially \u2013 \u2022 GFS or google file system \u2013 for storage \u2022 MapReduce \u2013 for processing Now, it gave another paper for quick searching \u2013 BigTable. Hbase is a distributed database management system, that runs on top of Hadoop. So, if you have 4 node Hadoop cluster, Hbase runs on this Hadoop cluster.","title":"Hadoop makes a very poor database, because ..."},{"location":"HBase/1-HBase/#hbase-properties","text":"\u2022 Distributed \u2013 stores data in HDFS \u2022 Scalable \u2013 capacity directly proportional to number of nodes in the cluster. \u2022 Fault tolerant \u2013 piggybacks on Hadoop. HDFS gives replication of data, and so same for HBase.","title":"Hbase properties"},{"location":"HBase/2-HBase/","text":"Hadoop is not a database. BigTable paper led to Hbase, which is a distributed database management system running on top of Hadoop. To recall 4 requirements of database are \u2013 1. Structured data 2. Low latency 3. Random reads 4. ACID compliance Let us see how Hbase fares against these requirements of database. 1. Structured \u2013 loose data structure. - HBase is not as rigid as traditional databases like MySQL. For example, all rows will have equal number of columns. 2. Low latency \u2013 real time access using row-based indices called row keys. 3. Random access \u2013 row keys allow access updates to one record. 4. Somewhat ACID compliant \u2013 some transactions will have ACID properties. Hbase can offer 2 things \u2013 1. Quick searching \u2013 based on row-keys. 2. Processing \u2013 mapreduce. Not the main use case as we already have Hive for this. Hbase Vs Relational Databases Properties of Hbase \u2013 1. Columnar store 2. Denormalized storage 3. Only CRUD operations 4. ACID at row level. Columnar storage Traditional databases provide row-based architecture, while Hbase provides column based 1 row is divided into many rows depending on number of columns. Advantage of columnar of stores \u2013 a. Sparse tables \u2013 if your table is sparse, no waste when storing sparse data. b. Dynamic attributes \u2013 update attributes dynamically without changing storage structure. Sparse Tables - In traditional databases, columns which have no values, they still occupy space even if no data is present in them. Thus, there is wastage of space. In Hbase, we handle sparse tables. Dynamic attributes \u2013 Unlike Hbase, in traditional databases changing column types/adding new columns/dropping them \u2013 all of this is a painful task. In columnar store of Hbase, each row can have a different number of columns. For example, in below image, we have \u2018expiry\u2019 column for id 1 and 4 only. Further no extra space \u2013 sparse table handled. Dynamically adding columns. We can have different columns with different rows. Denormalized Storage In traditional database, data is normalized. That is data is divided into multiple small tables. In Hbase, we don\u2019t prefer normalization. That means we store data into 1 big table. Advantage. \u2013 read a single record to get all details about a person in a single read operation. CRUD operations in Hbase In normal database, we have join operations. In Hbase, we can only perform CRUD operations. Join/groupBy/orderBy \u2013 not supported in Hbase. CRUD \u2013 create, read, update & delete. Note \u2013 since data is denormalized, so no need of Joins. This is why Hbase tables should be self-contained \u2013 i.e., contain all required info in 1 row. ACID Compliance at row level Traditional databases are ACID compliant. Hbase provides ACID compliance at row level. That means, \u2022 Whenever we are trying to update multiple columns for a single row, then either all of them will be updated or none will be updated. \u2022 When we talk about updates for multiple rows in Hbase then there is no guarantee. There could be a possibility that few rows got updated and rest not updated. \u2022 It also means, even if we are updating single column in multiple rows, there is no guarantee.","title":"HBase Properties"},{"location":"HBase/2-HBase/#hbase-vs-relational-databases","text":"Properties of Hbase \u2013 1. Columnar store 2. Denormalized storage 3. Only CRUD operations 4. ACID at row level.","title":"Hbase Vs Relational Databases"},{"location":"HBase/2-HBase/#columnar-storage","text":"Traditional databases provide row-based architecture, while Hbase provides column based 1 row is divided into many rows depending on number of columns. Advantage of columnar of stores \u2013 a. Sparse tables \u2013 if your table is sparse, no waste when storing sparse data. b. Dynamic attributes \u2013 update attributes dynamically without changing storage structure. Sparse Tables - In traditional databases, columns which have no values, they still occupy space even if no data is present in them. Thus, there is wastage of space. In Hbase, we handle sparse tables. Dynamic attributes \u2013 Unlike Hbase, in traditional databases changing column types/adding new columns/dropping them \u2013 all of this is a painful task. In columnar store of Hbase, each row can have a different number of columns. For example, in below image, we have \u2018expiry\u2019 column for id 1 and 4 only. Further no extra space \u2013 sparse table handled. Dynamically adding columns. We can have different columns with different rows.","title":"Columnar storage"},{"location":"HBase/2-HBase/#denormalized-storage","text":"In traditional database, data is normalized. That is data is divided into multiple small tables. In Hbase, we don\u2019t prefer normalization. That means we store data into 1 big table. Advantage. \u2013 read a single record to get all details about a person in a single read operation.","title":"Denormalized Storage"},{"location":"HBase/2-HBase/#crud-operations-in-hbase","text":"In normal database, we have join operations. In Hbase, we can only perform CRUD operations. Join/groupBy/orderBy \u2013 not supported in Hbase. CRUD \u2013 create, read, update & delete. Note \u2013 since data is denormalized, so no need of Joins. This is why Hbase tables should be self-contained \u2013 i.e., contain all required info in 1 row.","title":"CRUD operations in Hbase"},{"location":"HBase/2-HBase/#acid-compliance-at-row-level","text":"Traditional databases are ACID compliant. Hbase provides ACID compliance at row level. That means, \u2022 Whenever we are trying to update multiple columns for a single row, then either all of them will be updated or none will be updated. \u2022 When we talk about updates for multiple rows in Hbase then there is no guarantee. There could be a possibility that few rows got updated and rest not updated. \u2022 It also means, even if we are updating single column in multiple rows, there is no guarantee.","title":"ACID Compliance at row level"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/","text":"File Compression Techniques These techniques are common to Hadoop ecosystem, not just Hive. Why need compression? \u2022 Helps reduce storage especially when it comes to data being replicated across various nodes. \u2022 Helps us process data faster as size of data is less. \u2022 Since data is compressed, so I/O costs is less \u2013 A major overhead in processing large amounts of data is disk and network I/O, reducing the amount of data that needs to be read and written to disk can significantly decrease overall processing time. This includes compression of source data, but also the intermediate data generated as part of data processing. Compression and Decompression comes with some cost in terms of time taken to compress and decompress. But when we compare I/O gains, we can actually ignore this additional time to compress-decompress. Important Compression Techniques \u2013 1. Snappy 2. Lzo 3. Gzip 4. Bzip2 Some of the compression codecs are optimized for storage \u2013 they bring down size drastically. But this takes time. Some of compression codecs are optimized for speed \u2013 compression done quickly, but not efficiently. So trade-off is that \u2013 \u2022 if we want more compression ratio, we have to spend more time in compression. \u2022 If we want faster compression, we spend less time in compression. Snappy Snappy is a very fast compression. However, in terms of compression ratio, it is not that efficient. But in most production scenarios, snappy is used as it provides a fine balance between speed and compression efficiency. So, snappy is optimized for speed, not storage. Splittablity in compression techniques Although compression can greatly optimize processing performance, not all compression formats supported on Hadoop are splittable. Because the MapReduce framework splits data for input to multiple tasks, having a non splittable compression format is an impediment to efficient processing. If files cannot be split, that means the entire file needs to be passed to a single MapReduce task, eliminating the advantages of parallelism and data locality that Hadoop provides. For this reason, splitability is a major consideration in choosing a compression format as well as file format. Snappy by default is not splittable \u2013 so if we use non splittable file formats like JSON and XML, snappy won\u2019t give splittable output. Is this a big concern? No \u2013 because in production scenarios, we hardly use JSON and XML. In production scenarios, we use container-based formats like Avro, Parquet, Orc \u2013 which are splittable by their structure and no need for compression technique to handle this aspect. So, Snappy is intended to be used with a container format like Avro, Orc, Parquet since it\u2019s not inherently splittable. Lzo LZO is similar to Snappy in that it\u2019s optimized for speed as opposed to size. Unlike Snappy, LZO compressed files are splittable, but this requires an additional indexing step. This makes LZO a good choice for things like plain-text files (like json, text and xml files) that are not being stored as part of a container format. It should also be noted that LZO\u2019s license prevents it from being distributed with Hadoop and requires a separate install, unlike Snappy, which can be distributed with Hadoop. But snappy is fastest among all compression techniques. Gzip \u2022 Gzip provides very good compression performance (on average, about 2.5times the compression that\u2019d be offered by Snappy). \u2022 But in terms of processing speed its slow. \u2022 Gzip is also not splittable, so it should be used with a container format. \u2022 Note that one reason Gzip is sometimes slower than Snappy for processing is that Gzip compressed files take up fewer blocks, so fewer tasks are required for processing the same data. For this reason, using smaller blocks with Gzip can lead to better performance. \u2022 Eg \u2013 o 1 gb file \u2013 split into 8 blocks. o After gzip compression, we get 200 mb file \u2013 2 block \u2013 so number of blocks coming down, which reduces parallelism. o Solution \u2013 reduce block size to say, 50Mb, leading to 200mb file split into 4 blocks, and so parallelism doubles. Bzip2 \u2022 Bzip2 provides excellent compression performance, but can be significantly slower than other compression codecs such as Snappy in terms of processing performance. \u2022 Unlike Snappy and Gzip, bzip2 is inherently splittable. \u2022 In the examples we have seen, bzip2 will normally compress around 9% better than GZip, in terms of storage space. \u2022 However, this extra compression comes with a significant read/write performance cost. This performance difference will vary with different machines, but in general bzip2 is about 10 times slower than GZip. \u2022 For this reason, it\u2019s not an ideal codec for Hadoop storage, unless your primary need is reducing the storage footprint. One example of such a use case would be using Hadoop mainly for active archival purposes. Codec Splittable compression speed snappy N low Highest lzo Y low high gzip N High slow bzip2 Y Highest Slowest","title":"File Compression Techniques in Big Data Systems"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#file-compression-techniques","text":"These techniques are common to Hadoop ecosystem, not just Hive. Why need compression? \u2022 Helps reduce storage especially when it comes to data being replicated across various nodes. \u2022 Helps us process data faster as size of data is less. \u2022 Since data is compressed, so I/O costs is less \u2013 A major overhead in processing large amounts of data is disk and network I/O, reducing the amount of data that needs to be read and written to disk can significantly decrease overall processing time. This includes compression of source data, but also the intermediate data generated as part of data processing. Compression and Decompression comes with some cost in terms of time taken to compress and decompress. But when we compare I/O gains, we can actually ignore this additional time to compress-decompress. Important Compression Techniques \u2013 1. Snappy 2. Lzo 3. Gzip 4. Bzip2 Some of the compression codecs are optimized for storage \u2013 they bring down size drastically. But this takes time. Some of compression codecs are optimized for speed \u2013 compression done quickly, but not efficiently. So trade-off is that \u2013 \u2022 if we want more compression ratio, we have to spend more time in compression. \u2022 If we want faster compression, we spend less time in compression.","title":"File Compression Techniques"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#snappy","text":"Snappy is a very fast compression. However, in terms of compression ratio, it is not that efficient. But in most production scenarios, snappy is used as it provides a fine balance between speed and compression efficiency. So, snappy is optimized for speed, not storage.","title":"Snappy"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#splittablity-in-compression-techniques","text":"Although compression can greatly optimize processing performance, not all compression formats supported on Hadoop are splittable. Because the MapReduce framework splits data for input to multiple tasks, having a non splittable compression format is an impediment to efficient processing. If files cannot be split, that means the entire file needs to be passed to a single MapReduce task, eliminating the advantages of parallelism and data locality that Hadoop provides. For this reason, splitability is a major consideration in choosing a compression format as well as file format. Snappy by default is not splittable \u2013 so if we use non splittable file formats like JSON and XML, snappy won\u2019t give splittable output. Is this a big concern? No \u2013 because in production scenarios, we hardly use JSON and XML. In production scenarios, we use container-based formats like Avro, Parquet, Orc \u2013 which are splittable by their structure and no need for compression technique to handle this aspect. So, Snappy is intended to be used with a container format like Avro, Orc, Parquet since it\u2019s not inherently splittable.","title":"Splittablity in compression techniques"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#lzo","text":"LZO is similar to Snappy in that it\u2019s optimized for speed as opposed to size. Unlike Snappy, LZO compressed files are splittable, but this requires an additional indexing step. This makes LZO a good choice for things like plain-text files (like json, text and xml files) that are not being stored as part of a container format. It should also be noted that LZO\u2019s license prevents it from being distributed with Hadoop and requires a separate install, unlike Snappy, which can be distributed with Hadoop. But snappy is fastest among all compression techniques.","title":"Lzo"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#gzip","text":"\u2022 Gzip provides very good compression performance (on average, about 2.5times the compression that\u2019d be offered by Snappy). \u2022 But in terms of processing speed its slow. \u2022 Gzip is also not splittable, so it should be used with a container format. \u2022 Note that one reason Gzip is sometimes slower than Snappy for processing is that Gzip compressed files take up fewer blocks, so fewer tasks are required for processing the same data. For this reason, using smaller blocks with Gzip can lead to better performance. \u2022 Eg \u2013 o 1 gb file \u2013 split into 8 blocks. o After gzip compression, we get 200 mb file \u2013 2 block \u2013 so number of blocks coming down, which reduces parallelism. o Solution \u2013 reduce block size to say, 50Mb, leading to 200mb file split into 4 blocks, and so parallelism doubles.","title":"Gzip"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#bzip2","text":"\u2022 Bzip2 provides excellent compression performance, but can be significantly slower than other compression codecs such as Snappy in terms of processing performance. \u2022 Unlike Snappy and Gzip, bzip2 is inherently splittable. \u2022 In the examples we have seen, bzip2 will normally compress around 9% better than GZip, in terms of storage space. \u2022 However, this extra compression comes with a significant read/write performance cost. This performance difference will vary with different machines, but in general bzip2 is about 10 times slower than GZip. \u2022 For this reason, it\u2019s not an ideal codec for Hadoop storage, unless your primary need is reducing the storage footprint. One example of such a use case would be using Hadoop mainly for active archival purposes. Codec Splittable compression speed snappy N low Highest lzo Y low high gzip N High slow bzip2 Y Highest Slowest","title":"Bzip2"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/","text":"Text File format Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text-based File Formats"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/#text-file-format","text":"Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text File format"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/","text":"Avro, ORC and Parquet There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Big Data File Formats"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/#avro-orc-and-parquet","text":"There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Avro, ORC and Parquet"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/","text":"Row Vs Column File Format When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row-based Vs Column-based File Formats"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/#row-vs-column-file-format","text":"When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row Vs Column File Format"},{"location":"case%20study/IHS-Markit-System-Design/","text":"IHS Markit System Design Round Sharing the question below for reference. We are given 3 data sources -> 1. Azure SQL Database - 1. contains dealership data, with info -> 1. dealer_id 2. region 3. OEM Data 2. This data is updated once a week 2. MongoDb database - 1. Customer Profile data 1. customer_id 2. dealer_id 3. array of marketing history 2. This data is updated once a day 3. Pub/Sub queue messages containing Sales 1. updated in real time 2. contains sales information, with info -> 1. customer_id 2. dealer_id 3. timestamp of Sale Design a system to ingest data from multiple sources and report below Aggregates KPIs on Dashboard - aggregate sales total by Dealer, Month, Region, OEM etc","title":"IHS Markit System Design Case Study (2020)"},{"location":"case%20study/IHS-Markit-System-Design/#ihs-markit-system-design-round","text":"Sharing the question below for reference. We are given 3 data sources -> 1. Azure SQL Database - 1. contains dealership data, with info -> 1. dealer_id 2. region 3. OEM Data 2. This data is updated once a week 2. MongoDb database - 1. Customer Profile data 1. customer_id 2. dealer_id 3. array of marketing history 2. This data is updated once a day 3. Pub/Sub queue messages containing Sales 1. updated in real time 2. contains sales information, with info -> 1. customer_id 2. dealer_id 3. timestamp of Sale Design a system to ingest data from multiple sources and report below Aggregates KPIs on Dashboard - aggregate sales total by Dealer, Month, Region, OEM etc","title":"IHS Markit System Design Round"},{"location":"case%20study/Sample-Questions-System-Design/","text":"Meesho Round 1 Design a data platform. multiple sources and sinks. connectors for reading/writing data. purpose - 1. read data -> perform ETL (transform on sequence of rules - string commands in spark sql -> load into sink). configurable. define pipeline : a pipeline is bunch of similar events. event can be supplier data etc. events of similar data is from same source for example supplier data from kafka. Transformation are tied to events not pipeline. event 1 - T1-> T2 event 2 - T1 -> T4 -> T5 transformation are event specific. source and sink are pipeline specific. Also, what will be its Base classes and functions. Purpose of functions. Round 2 Consider an Ecommerce Website, with ClickStream events data being generated at the rate of 500k/second. These events can be add_to_cart, view, order, wishlisted, ... etc. The storage is Cloud Storage. Requirements - Build a data platform with following characteristics --> Self serve platform to provide ETL (hourly, daily, weekly etc) e.g. Product view count per hour per product, Product view count per day per product etc, User has ordered product after clicking on ad in last 3 days Adhoc queries/Notebook interface - Analysts (300 DAU) ML use cases (feature engineering, training etc - timetravel queries, historical data ) } hudi | deltalake Existing - 1500 - 2000 jobs (sql) 1. Might contain duplicate 2. Non optimized (No predicates/filters) What are things you will take care of for new jobs/sql? Glance / InMobi Round 1 There is a streaming service called Show-Stream ( like Netflix, Disney etc) that has customers spread over the globe. Show- Stream does not have a data warehouse in place. Design the pipeline to ingest the data from json events residing in S3 and build a Data Warehouse . How will you go about it? How do you go about deciding the tech stack ? Source S3://showstrteam/rawEvents/<date>/<hr>/file_n.json ( multiple events in one file, 100 such files) What are these events ? These events are all actions that the user performs on the app/browser :- user creation, playing a show, pausing a show, searching content etc etc. Round 2 We are given streaming input with information in format given below --> Input: UserID, follow/unfollow, recipeientUserID As an output you need to get 2 kinds of information - 1. List of users I follow 2. Count of users I follow Questions -> a. How will you capture this information? b. how will you store/transform the data? Uber System Design Interview Questions System Design competency. Below is Uber Schema - rider - rider id, name, age, gender, dob, phone, current payment type , current payment account; rider_bookmarks - bookmark id, rider_id, bookmark_tag('home','office' etc), bookmark_locn id; driver - driver id, name, joined date, curr cab id; check if running multiple vehicle so have curr cab id; cab - vehicle type, per_km, base_fare, etc; cab id, cab type, brand, reg no, year of make; map_grid = locn in a map grid of 1x1km; id, latitude,longitude; locn - human readable landmark locn - india gate, school, rly station etc; id, map_grid_id, is_landmark, related_locn_id, zip_code; trip - trip_id, cab id,rider id,start time, end time,request time, is surge applied, surge %, rider rating, driver rating, start locn id, end locn id; payment - id, type, base fare, surge fare, total fare, payment timestamp, card no, transaction id Question \u2013 System Design to handle scenario wherein customers who used Uber app to take a ride but couldn\u2019t subscribe to the ride. That is ,they drop off at the final tunnel. Economist Case Study The Economist Group\u2019s core business is our global news subscriptions business. Users subscribe to a weekly online magazine. They go through various stages : anonymous visitor ==> registered user ==> subscriber . All subscription information is stored in upstream transactional systems - Salesforce and Zuora . Different events can take place - cancellations, re-subscription/renewals. From time to time, the Group offers various promotions and offers in order to acquire new customers. The user behaviour data ( click stream info) is also available in Google Analytics. Design a data warehouse and associated data pipelines for the Economist Group so that the analysts and data scientists can report on key metrics. Document the 1. Architecture 2. Data flow, data model 3. Data observability 4. KPIs, metrics 5. Assumptions","title":"Interview Questions"},{"location":"case%20study/Sample-Questions-System-Design/#meesho","text":"","title":"Meesho"},{"location":"case%20study/Sample-Questions-System-Design/#round-1","text":"Design a data platform. multiple sources and sinks. connectors for reading/writing data. purpose - 1. read data -> perform ETL (transform on sequence of rules - string commands in spark sql -> load into sink). configurable. define pipeline : a pipeline is bunch of similar events. event can be supplier data etc. events of similar data is from same source for example supplier data from kafka. Transformation are tied to events not pipeline. event 1 - T1-> T2 event 2 - T1 -> T4 -> T5 transformation are event specific. source and sink are pipeline specific. Also, what will be its Base classes and functions. Purpose of functions.","title":"Round 1"},{"location":"case%20study/Sample-Questions-System-Design/#round-2","text":"Consider an Ecommerce Website, with ClickStream events data being generated at the rate of 500k/second. These events can be add_to_cart, view, order, wishlisted, ... etc. The storage is Cloud Storage. Requirements - Build a data platform with following characteristics --> Self serve platform to provide ETL (hourly, daily, weekly etc) e.g. Product view count per hour per product, Product view count per day per product etc, User has ordered product after clicking on ad in last 3 days Adhoc queries/Notebook interface - Analysts (300 DAU) ML use cases (feature engineering, training etc - timetravel queries, historical data ) } hudi | deltalake Existing - 1500 - 2000 jobs (sql) 1. Might contain duplicate 2. Non optimized (No predicates/filters) What are things you will take care of for new jobs/sql?","title":"Round 2"},{"location":"case%20study/Sample-Questions-System-Design/#glance-inmobi","text":"","title":"Glance / InMobi"},{"location":"case%20study/Sample-Questions-System-Design/#round-1_1","text":"There is a streaming service called Show-Stream ( like Netflix, Disney etc) that has customers spread over the globe. Show- Stream does not have a data warehouse in place. Design the pipeline to ingest the data from json events residing in S3 and build a Data Warehouse . How will you go about it? How do you go about deciding the tech stack ? Source S3://showstrteam/rawEvents/<date>/<hr>/file_n.json ( multiple events in one file, 100 such files) What are these events ? These events are all actions that the user performs on the app/browser :- user creation, playing a show, pausing a show, searching content etc etc.","title":"Round 1"},{"location":"case%20study/Sample-Questions-System-Design/#round-2_1","text":"We are given streaming input with information in format given below --> Input: UserID, follow/unfollow, recipeientUserID As an output you need to get 2 kinds of information - 1. List of users I follow 2. Count of users I follow Questions -> a. How will you capture this information? b. how will you store/transform the data?","title":"Round 2"},{"location":"case%20study/Sample-Questions-System-Design/#uber-system-design-interview-questions","text":"System Design competency. Below is Uber Schema - rider - rider id, name, age, gender, dob, phone, current payment type , current payment account; rider_bookmarks - bookmark id, rider_id, bookmark_tag('home','office' etc), bookmark_locn id; driver - driver id, name, joined date, curr cab id; check if running multiple vehicle so have curr cab id; cab - vehicle type, per_km, base_fare, etc; cab id, cab type, brand, reg no, year of make; map_grid = locn in a map grid of 1x1km; id, latitude,longitude; locn - human readable landmark locn - india gate, school, rly station etc; id, map_grid_id, is_landmark, related_locn_id, zip_code; trip - trip_id, cab id,rider id,start time, end time,request time, is surge applied, surge %, rider rating, driver rating, start locn id, end locn id; payment - id, type, base fare, surge fare, total fare, payment timestamp, card no, transaction id Question \u2013 System Design to handle scenario wherein customers who used Uber app to take a ride but couldn\u2019t subscribe to the ride. That is ,they drop off at the final tunnel.","title":"Uber System Design Interview Questions"},{"location":"case%20study/Sample-Questions-System-Design/#economist-case-study","text":"The Economist Group\u2019s core business is our global news subscriptions business. Users subscribe to a weekly online magazine. They go through various stages : anonymous visitor ==> registered user ==> subscriber . All subscription information is stored in upstream transactional systems - Salesforce and Zuora . Different events can take place - cancellations, re-subscription/renewals. From time to time, the Group offers various promotions and offers in order to acquire new customers. The user behaviour data ( click stream info) is also available in Google Analytics. Design a data warehouse and associated data pipelines for the Economist Group so that the analysts and data scientists can report on key metrics. Document the 1. Architecture 2. Data flow, data model 3. Data observability 4. KPIs, metrics 5. Assumptions","title":"Economist Case Study"},{"location":"case%20study/ZS-Associates-Data-Consultant-Case-Study/","text":"ZS Associates Data Solution Consultant Case Study","title":"ZS Associates Data Consultant System Design Case Study (2020)"},{"location":"case%20study/ZS-Associates-Data-Consultant-Case-Study/#zs-associates-data-solution-consultant-case-study","text":"","title":"ZS Associates Data Solution Consultant Case Study"},{"location":"case%20study/design%20uber/","text":"Requirements 3 high level requirememts. - users should be able to raise a request for cab. - match customer to driver who is nearby. - route cab is going to follow, and associated ETA. Think at very high level, our 2 main users are - - taxi driver - tell availability through web - client - makes request for cab through web. need a matching system to match user and drievr. we sgould have maps/eta service - to tell route to be followed and what is its ETA. store information in database. system to do analytcis - answer business quetsion like where cabs are most located, who are my prime custoemrs, fraud etc. Source: Data Savvy","title":"Design uber"},{"location":"case%20study/uber-big-data-platform/","text":"In addition to incorporating a Hadoop data lake, we also made all data services in this ecosystem horizontally scalable, thereby improving the efficiency and stability of our Big Data platform. Unlike the first generation of our platform in which data pipelines were vulnerable to upstream data format changes, our second iteration allowed us to schematize all data, transitioning from JSON to Parquet to store schema and data together. Limitations 1. small file problem - massive amount of small files stored in our HDFS 2. latency = HIGH - data latency was still far from what our business needed. New data was only accessible to users once every 24 hours, which was too slow to make real-time decisions. 3. prcoess whole day snapshot again - both ingestion of the new data and modeling of the related derived table were based on creating new snapshots of the entire dataset and swapping the old and new tables to provide users with access to fresh data. The ingestion jobs had to return to the source datastore, create a new snapshot, and ingest or convert the entire dataset into consumable, columnar Parquet files during every run. With our data stores growing, these jobs could take over twenty hours with over 1,000 Spark executors to run. 4. upsert not available - reprocess whole days. Uber architecture gen3 Limitations of existing system 1. HDFS scalability limitation 2. need something Faster data in Hadoop - data in real-time 3. Support of updates and deletes in Hadoop and Parquet needed. 4. Faster ETL and modeling: need upsert processing in ETL pipeline Solution - 1. Hudi - 1. update, insert, and delete existing Parquet data in Hadoop. 2. allows data users to incrementally pull out only changed data, significantly improving query efficiency and allowing for incremental updates of derived modeled tables. 2. Generic data ingestion using Kafka & Marmary , a data ingestion platform at Uber. all streaming data ingested using Kafka and loaded using marmary. Why Kafka - It acts as unified ingestion service wherein all upstream datastore events (as well as classic logging messages from different applications and services) stream into Kafka with a unified Avro encoding including standard global metadata headers attached (i.e., timestamp, row key, version, data center information, and originating host). Both the Streaming and Big Data teams use these storage changelog events as their source input data for further processing. Marmary - It runs in mini-batches and picks up the upstream storage changelogs from Kafka, applying them on top of the existing data in Hadoop using Hudi library.","title":"Uber big data platform"}]}