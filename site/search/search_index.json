{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Big Data Architecture Topics Big data Architectural Patterns - Introduction - Big Data Architectural Principles - Data Collection Layer - Data Processing Layer - Data Consumption Layer - Putting it all together Storage Layer - Row-based Vs Column-based File Formats - Text-based File Formats - Big Data File Formats - File Compression Techniques in Big Data Systems Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture","title":"Home"},{"location":"#welcome-to-big-data-architecture","text":"","title":"Welcome to Big Data Architecture"},{"location":"#topics","text":"Big data Architectural Patterns - Introduction - Big Data Architectural Principles - Data Collection Layer - Data Processing Layer - Data Consumption Layer - Putting it all together Storage Layer - Row-based Vs Column-based File Formats - Text-based File Formats - Big Data File Formats - File Compression Techniques in Big Data Systems Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture","title":"Topics"},{"location":"aboutme/","text":"About me Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData & Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos. He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks. Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.","title":"About Me"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/","text":"Big Data Analytics Architectural Patterns and Best Practices Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/#big-data-analytics-architectural-patterns-and-best-practices","text":"Topics -> - Big Data Challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/","text":"There are different kind of analytical systems. Some customers are looking at doing batch-interactive analytics with their data. Others are looking at being able to to take real time data feeds, and store them for insights, and some go beyond, seking building of data models on top of it, power inferencing and do ML with that data. As we step step into the architecture discussion, we are going to discuss architecture for -> - Streaming processing - datalakes and batch interaction - machine learning. Note - you dont need to pick them right away. You can start small, architect it in a way that it enables you to build more and more features on top of it over time, incrementally. Now, lets look at different model of delivering big data services. Virtualized - eg- EC2 instances that we create & install kafka on top of it. Customer owns environment and manages it themselves. Managed Services - EMR (hadoop platform as a service), RDS - managed by AWS - customers still thinking about requirements like what configuration you need, what should be auto-scaling policy etc. Serverless/Clusterless/Containerized - Lambda, Athena, Glue - these are services that abstract out the servers away from you. This means users can focus on core use case instead of being caught up in acquiring cluster & configuring it, installing software etc. Various services - both open source and AWS based are mentioned below - Big Data Challenges Lets start with the challenge first. As a customer, one would like to have answers to questions like -> - Is there a reference architecture for my use case? - If yes, what tools should one choose? - How? - Why?","title":"Introduction"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/#big-data-challenges","text":"Lets start with the challenge first. As a customer, one would like to have answers to questions like -> - Is there a reference architecture for my use case? - If yes, what tools should one choose? - How? - Why?","title":"Big Data Challenges"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/","text":"Big Data Architectural Principles Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages. A simplified data processing pipeline Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders. What is the temperature of my data? Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#big-data-architectural-principles","text":"Before we discuss the answer to Big Data Challenges, lets discuss the key architectural principles. Build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. As there is separation of concerns, it allows you to iterate over each and every sub-system, and so you could truly build and eveolve over time. This makes you future-proof . This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares, ensure their upgrade. etc. This is all handled by vendor behind the scenes. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. When you want to build analytical systems, use immutable datasets (i.e. Data lake) where you want to capture State of an application in order to load as materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#a-simplified-data-processing-pipeline","text":"Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. It's not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders.","title":"A simplified data processing pipeline"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#what-is-the-temperature-of-my-data","text":"Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"What is the temperature of my data?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/","text":"Collect Layer Types of data sources In Collection, you need to figure out the type of data you are collecting . Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NoSQL database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. Transactional data For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). Files/Objects data For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed. Stream Storage AWS-based Stream Storage options For stream storage, we have 3 main options in AWS -> Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify the end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper. Apache Kafka : Stream Storage In open source we have Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2. Which Streaming/Message Storage should I use? SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage. Which File/Object Storage should I use? Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data. S3 as Datalake storage One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. By having your storage outside the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involve starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. This makes S3 durable. Security - it used encryption at rest and in transit, both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section). Data Tiering & S3 cost optimization strategy The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3. What about Metadata? In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. We have 3 options -> AWS Glue Catalog It is a fully managed data catalog , and is Hive metastore compliant . Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify the metadata for you so that you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions. Lakehouse Formation When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, find matches where there is no matching id, etc. Hive Metastore If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS. Cache & Database Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache, Redis - DynamoDB Accelerator Graph DB Amazon Neptune , neo4j Key-Value document Amazon DynamoDB , Cassandra SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard. Which database store should I use? Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Data Collection Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#collect-layer","text":"","title":"Collect Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#types-of-data-sources","text":"In Collection, you need to figure out the type of data you are collecting . Often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NoSQL database. Often times these are records that you need to be able to analyze and process. They come in the form of Web applications, Mobile Applications, Data Centres, etc. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different.","title":"Types of data sources"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#transactional-data","text":"For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this).","title":"Transactional data"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#filesobjects-data","text":"For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed.","title":"Files/Objects data"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#stream-storage","text":"","title":"Stream Storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#aws-based-stream-storage-options","text":"For stream storage, we have 3 main options in AWS -> Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard processes a 1000 records/seconds. Say, you want to scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. - Thus, Kinesis is truly serverless . Where you provision your streams, and specify only shards, and not resources etc. - Kinesis allows real-time analytics. Amazon Kinesis Data Firehose - Managed data delivery - Let's say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline processing of that data. - Here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be able to store that data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destinations in order to store the data as the data is flowing in. Think of it as a pipe wherein you specify the end-point and that becomes your target location from where you fetch data for your analytics. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Amazon Managed Streaming for Apache Kafka (Amazon MSK) - This is Amazon managed Kafka service, wherein you can setup kafka cluster with a few clicks. - Here AWS manages Kafka cluster, including Zookeeper.","title":"AWS-based Stream Storage options"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#apache-kafka-stream-storage","text":"In open source we have Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2.","title":"Apache Kafka : Stream Storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-streamingmessage-storage-should-i-use","text":"SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage.","title":"Which Streaming/Message Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-fileobject-storage-should-i-use","text":"Here S3/HDFS is the defacto standard. S3 allows you to store and retrieve any amount of data.","title":"Which File/Object Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#s3-as-datalake-storage","text":"One of the main use case for S3 is to use it as the centre of your data lake. There are a number of reasons for it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute S3 allows you to decouple storage and compute, and this is really powerful. By having your storage outside the hadoop ecosystem, it allows you to run transient clusters over your dataset. Transient clusters are short-lived. You could introduce steps that involve starting your cluster, processing your data, write output to S3, and terminate the cluster. What is the benefit here? First, Cost - you pay for what you use. Second, it gives scope for innovation & Flexibility. For example, multiple & heterogenous anlaysis clusters and services can use the same data. In case of spark processing, you could use EC2 instances which are memory optimized. At the same time, you could also have GPUs to run tensorflow and ML on same dataset. You could run spark on EMR(based on reserved instances), and other for GPUs (based on spot instances). Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. This makes S3 durable. Security - it used encryption at rest and in transit, both. Low Cost - its cost effective. One of the ways to optimize S3 is to use Data Tiers (discussed in next section).","title":"S3 as Datalake storage"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#data-tiering-s3-cost-optimization-strategy","text":"The data that you are accessing frequently should be placed in Amazon S3. S3 gives you the option ( S3 Standard-IA ) to move your data to move data infrequent access layers, and this is where you save cost. There is also Amazon Glacier which is a cold storage, and is the data not available for doing analytics - data is stored in Archive. (In S3 Standard-IA, you can do analytics, but here the pricing works frequently.) If I have data in s3, do I still need HDFS? Maybe. One can store their working datasets for analysis in HDFS (in EMR cluster) for faster access instead of accessing them from S3.","title":"Data Tiering &amp; S3 cost optimization strategy"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#what-about-metadata","text":"In order to fully decouple your storage from compute, data is one part of equation, and metadata is other part. You need to have both outside the cluster in order to use the transient EMR cluster. We have 3 options ->","title":"What about Metadata?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#aws-glue-catalog","text":"It is a fully managed data catalog , and is Hive metastore compliant . Search metadata discovery - What is the purpose of Glue Catalog? - It aims to serve as Unified Data Catalog across all the sources on which you want to do data catalog on. Amazon Athena, Amazon EMR, Amazon Redshift Spectrum - all these are integrated with the Glue Catalog. Glue also has utility - Crawlers - which is an application that you can point to your data sources (S3, RDBMS etc), and it will explore the data, and will try to identify the metadata for you so that you create those tables to make it easier to find those schemas. So Crawlers used to Detect new data, schema, partitions.","title":"AWS Glue Catalog"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#lakehouse-formation","text":"When it comes to datalake, you still have to go to a few places to create - S3 (storage), Metadata(Glue Catalog), IAM (security access), etc. Lake-formation comes to help you to create a Datalake in AWS within a few minutes. It enhances those services with additional metadata, and will also introduce additional data quality jobs that can do task like de-duplication, find matches where there is no matching id, etc.","title":"Lakehouse Formation"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#hive-metastore","text":"If you don't want to go with AWS Glue, then can host your own Hive Metastore on Amazon RDS.","title":"Hive Metastore"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#cache-database","text":"Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache, Redis - DynamoDB Accelerator Graph DB Amazon Neptune , neo4j Key-Value document Amazon DynamoDB , Cassandra SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. From Analytics point of view, Database can be source of data from which we pull the data for doing analytics. It can also be target of your analytics, wherein you want to push results of your analytics to your database so that you can have some real-time dashboard.","title":"Cache &amp; Database"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-database-store-should-i-use","text":"Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? Main 2 questions are about what is my data structure and how I will access my Data. What is the data structure? - Do I have a very fixed schema? Then SQL is the way to go, especially you have complex relationships involving Table joins, and this is the way you access the data. - If your data is not having fixed schema, you should consider NoSQL database. - If your latency requirements is sub-seconds, then you should consider having in-memory Cache in front of your database/NoSQL system. - is it very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Which database store should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/","text":"Data Processing Layer We will highlight various available services in AWS and their use cases. Interactive & Batch Analytics First, let's look at interactive & batch analytics. Amazon Elastic Search Service Ir is a managed service for Elastic Search. Elastic Search clusters can be spun up in minutes. Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. Amazon Redshift & Redshift Spectrum for data warehousing needs. Redhsift is a fully managed Data Warehouse. AWS Redshift is an OLAP system. Its an MPP (Massively Parallel Processing) System . It will allow us to query terabytes of data. Its a schema on read kind of system, wherein you define Schema first and put in a highly structured data. Redshift Spectrum enables querying S3. Amazon Athena ~ Hive It is Serverless interactive query service. Performs SQL based queries on Data that resides on S3. Similar to Presto/Hive. S3 Select is somewhat similar to Athena, but there are some differences. You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. Athena works on logical object selecting its information from AWS Glue Catalog rather than selecting data out of an object on S3. S3 Select works on object out of S3 bucket. Amazon EMR Its Hadoop & Spark as a Service that lets you run various platform applications. one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it. Streaming/Real-time Analytics In Streaming analytics we have a number of options. 1. Spark Streaming on Amazon EMR. 2. Amazon Kinesis Data Analytics - 1. Its ia managed service for running SQL on streaming data. 3. Amazon KCL 1. You can always create your own application by using Amazon Kinesis Client Library. 4. Amazon Lambda 1. Run code serverless (without provisioning or managing servers). 2. Services such as S3 can publish events to AWS Lambda. 3. AWS Lambda can pool events from a Kinesis. If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data, and under the hood it will be working on real-time data feeds, and sending back the output. As a customer one key question that needs to be answered while picking a tool is - 'What is my end to end latency from point at which I publish a message to point at which I process the message and get final results?' AWS Lambda lets you write functions, and set up an event trigger for Kinesis off of that. But, in respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL. Predictive Analytics Here you have the option to have 3 layers - 1. Frameworks 2. Platforms 3. Application Services Frameworks It is for ML Practitioners, wherein you get the bare machines with those frameworks pre-installed, and you have to do traditional Machine Learning tasks like hyper-parameter tuning, etc. Platforms This layer allows us to build models and test them. Helps ML experts to dod tasks like giving them jupyter notebooks, familiar interfacce, and then you can use Sagemaker API to create instance models and then you can deploy the model. Its one kind of place where you can do all ML tasks on the platform. Application Services It is for developers who want to use ML in their aplication, but dont want to implement ML So, we have some algorithm which are successful like NLP, and these can be integrated by calling these APIs. Application services help built different types of ML algos on the platform. One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case. Which Analytics should I use? Batch analytics - think of it as a report that runs daily/weekly/monthly. Tools - Amazon EMR, Hive/Spark. Interactive Analytics See it is a s self-service dashboard, wherein you want to get answers yourself from the system. Tools - Redshift, Athena, Amzon EMR on Spark/Presto. Stream Analytics In EMR, you can utilize Spark Streaming, Kinesis Data analytics services. Predictive Analytics It can be batch or streaming. ETL Often times data comes in raw form, and what you want to do is to create curated datasets or canonical datasets, that represent a normalized view of data. And that's where ETL process comes in. And that normalized dataset is often in format different from the one that came in. So, your raw data could be in JSON, CSV etc, and your output would often be a column oriented format(parquet/orc), or row-based format(Avro - handles schema changes better). One concern around ETL is that one loses value of data. But in truth, ETL helps transform data in format that is more suitable for doing analytics - partitioning, converting form text to parquert/orc. Below are AWS ETL options. AWS Glue ETL Allows running spark/python shell. it is a serverless category wherein you abstract server complexity from user. you define data processing unit and schedule those jobs to run. Amazon Data Migration Service(DMS) If you want to have CDC (Change Data Capture), then Amazon Data Migration Service(DMS) is an option. Glue isn't abl;e to do that, or you can look at CDC solution from partner services.","title":"Data Processing Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#data-processing-layer","text":"We will highlight various available services in AWS and their use cases.","title":"Data Processing Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#interactive-batch-analytics","text":"First, let's look at interactive & batch analytics. Amazon Elastic Search Service Ir is a managed service for Elastic Search. Elastic Search clusters can be spun up in minutes. Strongest use case is Log Analysis, wherein you have dashboard based on ELK (elastic search & Kibana) stack. Other use case is in datalake is metadata indexing. Since Elastic Search is a search, it will allow you query with some search - give search experience your queries. You could index your metadata to elastic search, and have an additional search ability over your data sources in a datalake. Amazon Redshift & Redshift Spectrum for data warehousing needs. Redhsift is a fully managed Data Warehouse. AWS Redshift is an OLAP system. Its an MPP (Massively Parallel Processing) System . It will allow us to query terabytes of data. Its a schema on read kind of system, wherein you define Schema first and put in a highly structured data. Redshift Spectrum enables querying S3. Amazon Athena ~ Hive It is Serverless interactive query service. Performs SQL based queries on Data that resides on S3. Similar to Presto/Hive. S3 Select is somewhat similar to Athena, but there are some differences. You can think about AWS S3 Select as a cost-efficient storage optimization that allows retrieving data that matches the predicate in S3 and glacier aka push down filtering. AWS Athena is fully managed analytical service that allows running arbitrary ANSI SQL compliant queries - group by, having, window and geo functions, SQL DDL and DML. Athena works on logical object selecting its information from AWS Glue Catalog rather than selecting data out of an object on S3. S3 Select works on object out of S3 bucket. Amazon EMR Its Hadoop & Spark as a Service that lets you run various platform applications. one thing special about EMR is that you have root access to EC2 instances that EMR is using unlike other managed services. eg - in RDS, you ENIN point that allows you to perform database functions, but you cannot do SSH on it.","title":"Interactive &amp; Batch Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#streamingreal-time-analytics","text":"In Streaming analytics we have a number of options. 1. Spark Streaming on Amazon EMR. 2. Amazon Kinesis Data Analytics - 1. Its ia managed service for running SQL on streaming data. 3. Amazon KCL 1. You can always create your own application by using Amazon Kinesis Client Library. 4. Amazon Lambda 1. Run code serverless (without provisioning or managing servers). 2. Services such as S3 can publish events to AWS Lambda. 3. AWS Lambda can pool events from a Kinesis. If you look at Hadoop ecosystem, the number of services that perform real-time analytics has been growing - Spark Streaming, Flink, Storm. Just like Athena allows you to perform SQL queries on S3 even though it is not a relational database, AWS Kinesis Data Analytics allows you to perform SQL queries on your real time data even though it is not really a database. So you could write SQL - tumbling windows, random cut forest, different sort of expressions to analyze data, and under the hood it will be working on real-time data feeds, and sending back the output. As a customer one key question that needs to be answered while picking a tool is - 'What is my end to end latency from point at which I publish a message to point at which I process the message and get final results?' AWS Lambda lets you write functions, and set up an event trigger for Kinesis off of that. But, in respect to AWS Lambda, one thing to note is that it polls every second. That means if you are looking for sub-second latency, use other tool like Amazon KCL.","title":"Streaming/Real-time Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#predictive-analytics","text":"Here you have the option to have 3 layers - 1. Frameworks 2. Platforms 3. Application Services Frameworks It is for ML Practitioners, wherein you get the bare machines with those frameworks pre-installed, and you have to do traditional Machine Learning tasks like hyper-parameter tuning, etc. Platforms This layer allows us to build models and test them. Helps ML experts to dod tasks like giving them jupyter notebooks, familiar interfacce, and then you can use Sagemaker API to create instance models and then you can deploy the model. Its one kind of place where you can do all ML tasks on the platform. Application Services It is for developers who want to use ML in their aplication, but dont want to implement ML So, we have some algorithm which are successful like NLP, and these can be integrated by calling these APIs. Application services help built different types of ML algos on the platform. One thing is to note that make sure to write your data in open standards like orc, parquet, CSV or JSON. That ways you can use different tools with it for your use case.","title":"Predictive Analytics"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#which-analytics-should-i-use","text":"Batch analytics - think of it as a report that runs daily/weekly/monthly. Tools - Amazon EMR, Hive/Spark. Interactive Analytics See it is a s self-service dashboard, wherein you want to get answers yourself from the system. Tools - Redshift, Athena, Amzon EMR on Spark/Presto. Stream Analytics In EMR, you can utilize Spark Streaming, Kinesis Data analytics services. Predictive Analytics It can be batch or streaming.","title":"Which Analytics should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#etl","text":"Often times data comes in raw form, and what you want to do is to create curated datasets or canonical datasets, that represent a normalized view of data. And that's where ETL process comes in. And that normalized dataset is often in format different from the one that came in. So, your raw data could be in JSON, CSV etc, and your output would often be a column oriented format(parquet/orc), or row-based format(Avro - handles schema changes better). One concern around ETL is that one loses value of data. But in truth, ETL helps transform data in format that is more suitable for doing analytics - partitioning, converting form text to parquert/orc. Below are AWS ETL options.","title":"ETL"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#aws-glue-etl","text":"Allows running spark/python shell. it is a serverless category wherein you abstract server complexity from user. you define data processing unit and schedule those jobs to run.","title":"AWS Glue ETL"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/#amazon-data-migration-servicedms","text":"If you want to have CDC (Change Data Capture), then Amazon Data Migration Service(DMS) is an option. Glue isn't abl;e to do that, or you can look at CDC solution from partner services.","title":"Amazon Data Migration Service(DMS)"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-5/","text":"Data Consumption Process When it comes to consumption, we have 2 categories of users - 1. Business users - who want to make sense of data. The service here involved is applications like visualization applications like Tableau and Amazon Quicksight (amazon managed visualization tool), Kibana (visualization on elastic search). Data Scientist - They want to get access to an endpoint and play with data. They would like to use Athena, Redshift, etc. We could ask questions like what type of BI or UI one could use in this type of solution. Answer depends on who is user and what is the function they are doing. Eg - Business user would not prefer to be asked to use Jupyter notebooks, nor would a data scientist enjoy being placed in front of dashboard with little flexibility to play around with data. In this stage, solutions you would find would be a suite of different UI being used to be able to perform these analytics. Jupyter - used in data science space. Sagemaker allows you to have a managed Jupyter environment, running both Jupyter and Jupyter Lab. You could also run managed Jupyter or managed notebooks on EMR. So, if you are looking at just Hadoop ecosystem, we have option of EMR or managed hadoop environment. ELK Stack - Kibana can allow doing Log analysis. There are also partner products like Splunk, Tableau, Looker and MircoStrategy.","title":"Data Consumption Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-5/#data-consumption-process","text":"When it comes to consumption, we have 2 categories of users - 1. Business users - who want to make sense of data. The service here involved is applications like visualization applications like Tableau and Amazon Quicksight (amazon managed visualization tool), Kibana (visualization on elastic search). Data Scientist - They want to get access to an endpoint and play with data. They would like to use Athena, Redshift, etc. We could ask questions like what type of BI or UI one could use in this type of solution. Answer depends on who is user and what is the function they are doing. Eg - Business user would not prefer to be asked to use Jupyter notebooks, nor would a data scientist enjoy being placed in front of dashboard with little flexibility to play around with data. In this stage, solutions you would find would be a suite of different UI being used to be able to perform these analytics. Jupyter - used in data science space. Sagemaker allows you to have a managed Jupyter environment, running both Jupyter and Jupyter Lab. You could also run managed Jupyter or managed notebooks on EMR. So, if you are looking at just Hadoop ecosystem, we have option of EMR or managed hadoop environment. ELK Stack - Kibana can allow doing Log analysis. There are also partner products like Splunk, Tableau, Looker and MircoStrategy.","title":"Data Consumption Process"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-6/","text":"Putting it all together We have data from various sources flowing in. Different repositories based on requirements that are needed to store that data. One key call-outs we need to take note of is ETL process - How can I take my data, create it in the most consumable method for different types of users to be able to process and analyze that data, and ultimately getting it to derive insights and answer business questions, etc.","title":"Putting it all together"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-6/#putting-it-all-together","text":"We have data from various sources flowing in. Different repositories based on requirements that are needed to store that data. One key call-outs we need to take note of is ETL process - How can I take my data, create it in the most consumable method for different types of users to be able to process and analyze that data, and ultimately getting it to derive insights and answer business questions, etc.","title":"Putting it all together"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/","text":"File Compression Techniques These techniques are common to Hadoop ecosystem, not just Hive. Why need compression? \u2022 Helps reduce storage especially when it comes to data being replicated across various nodes. \u2022 Helps us process data faster as size of data is less. \u2022 Since data is compressed, so I/O costs is less \u2013 A major overhead in processing large amounts of data is disk and network I/O, reducing the amount of data that needs to be read and written to disk can significantly decrease overall processing time. This includes compression of source data, but also the intermediate data generated as part of data processing. Compression and Decompression comes with some cost in terms of time taken to compress and decompress. But when we compare I/O gains, we can actually ignore this additional time to compress-decompress. Important Compression Techniques \u2013 1. Snappy 2. Lzo 3. Gzip 4. Bzip2 Some of the compression codecs are optimized for storage \u2013 they bring down size drastically. But this takes time. Some of compression codecs are optimized for speed \u2013 compression done quickly, but not efficiently. So trade-off is that \u2013 \u2022 if we want more compression ratio, we have to spend more time in compression. \u2022 If we want faster compression, we spend less time in compression. Snappy Snappy is a very fast compression. However, in terms of compression ratio, it is not that efficient. But in most production scenarios, snappy is used as it provides a fine balance between speed and compression efficiency. So, snappy is optimized for speed, not storage. Splittablity in compression techniques Although compression can greatly optimize processing performance, not all compression formats supported on Hadoop are splittable. Because the MapReduce framework splits data for input to multiple tasks, having a non splittable compression format is an impediment to efficient processing. If files cannot be split, that means the entire file needs to be passed to a single MapReduce task, eliminating the advantages of parallelism and data locality that Hadoop provides. For this reason, splitability is a major consideration in choosing a compression format as well as file format. Snappy by default is not splittable \u2013 so if we use non splittable file formats like JSON and XML, snappy won\u2019t give splittable output. Is this a big concern? No \u2013 because in production scenarios, we hardly use JSON and XML. In production scenarios, we use container-based formats like Avro, Parquet, Orc \u2013 which are splittable by their structure and no need for compression technique to handle this aspect. So, Snappy is intended to be used with a container format like Avro, Orc, Parquet since it\u2019s not inherently splittable. Lzo LZO is similar to Snappy in that it\u2019s optimized for speed as opposed to size. Unlike Snappy, LZO compressed files are splittable, but this requires an additional indexing step. This makes LZO a good choice for things like plain-text files (like json, text and xml files) that are not being stored as part of a container format. It should also be noted that LZO\u2019s license prevents it from being distributed with Hadoop and requires a separate install, unlike Snappy, which can be distributed with Hadoop. But snappy is fastest among all compression techniques. Gzip \u2022 Gzip provides very good compression performance (on average, about 2.5times the compression that\u2019d be offered by Snappy). \u2022 But in terms of processing speed its slow. \u2022 Gzip is also not splittable, so it should be used with a container format. \u2022 Note that one reason Gzip is sometimes slower than Snappy for processing is that Gzip compressed files take up fewer blocks, so fewer tasks are required for processing the same data. For this reason, using smaller blocks with Gzip can lead to better performance. \u2022 Eg \u2013 o 1 gb file \u2013 split into 8 blocks. o After gzip compression, we get 200 mb file \u2013 2 block \u2013 so number of blocks coming down, which reduces parallelism. o Solution \u2013 reduce block size to say, 50Mb, leading to 200mb file split into 4 blocks, and so parallelism doubles. Bzip2 \u2022 Bzip2 provides excellent compression performance, but can be significantly slower than other compression codecs such as Snappy in terms of processing performance. \u2022 Unlike Snappy and Gzip, bzip2 is inherently splittable. \u2022 In the examples we have seen, bzip2 will normally compress around 9% better than GZip, in terms of storage space. \u2022 However, this extra compression comes with a significant read/write performance cost. This performance difference will vary with different machines, but in general bzip2 is about 10 times slower than GZip. \u2022 For this reason, it\u2019s not an ideal codec for Hadoop storage, unless your primary need is reducing the storage footprint. One example of such a use case would be using Hadoop mainly for active archival purposes. Codec Splittable compression speed snappy N low Highest lzo Y low high gzip N High slow bzip2 Y Highest Slowest","title":"File Compression Techniques in Big Data Systems"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#file-compression-techniques","text":"These techniques are common to Hadoop ecosystem, not just Hive. Why need compression? \u2022 Helps reduce storage especially when it comes to data being replicated across various nodes. \u2022 Helps us process data faster as size of data is less. \u2022 Since data is compressed, so I/O costs is less \u2013 A major overhead in processing large amounts of data is disk and network I/O, reducing the amount of data that needs to be read and written to disk can significantly decrease overall processing time. This includes compression of source data, but also the intermediate data generated as part of data processing. Compression and Decompression comes with some cost in terms of time taken to compress and decompress. But when we compare I/O gains, we can actually ignore this additional time to compress-decompress. Important Compression Techniques \u2013 1. Snappy 2. Lzo 3. Gzip 4. Bzip2 Some of the compression codecs are optimized for storage \u2013 they bring down size drastically. But this takes time. Some of compression codecs are optimized for speed \u2013 compression done quickly, but not efficiently. So trade-off is that \u2013 \u2022 if we want more compression ratio, we have to spend more time in compression. \u2022 If we want faster compression, we spend less time in compression.","title":"File Compression Techniques"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#snappy","text":"Snappy is a very fast compression. However, in terms of compression ratio, it is not that efficient. But in most production scenarios, snappy is used as it provides a fine balance between speed and compression efficiency. So, snappy is optimized for speed, not storage.","title":"Snappy"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#splittablity-in-compression-techniques","text":"Although compression can greatly optimize processing performance, not all compression formats supported on Hadoop are splittable. Because the MapReduce framework splits data for input to multiple tasks, having a non splittable compression format is an impediment to efficient processing. If files cannot be split, that means the entire file needs to be passed to a single MapReduce task, eliminating the advantages of parallelism and data locality that Hadoop provides. For this reason, splitability is a major consideration in choosing a compression format as well as file format. Snappy by default is not splittable \u2013 so if we use non splittable file formats like JSON and XML, snappy won\u2019t give splittable output. Is this a big concern? No \u2013 because in production scenarios, we hardly use JSON and XML. In production scenarios, we use container-based formats like Avro, Parquet, Orc \u2013 which are splittable by their structure and no need for compression technique to handle this aspect. So, Snappy is intended to be used with a container format like Avro, Orc, Parquet since it\u2019s not inherently splittable.","title":"Splittablity in compression techniques"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#lzo","text":"LZO is similar to Snappy in that it\u2019s optimized for speed as opposed to size. Unlike Snappy, LZO compressed files are splittable, but this requires an additional indexing step. This makes LZO a good choice for things like plain-text files (like json, text and xml files) that are not being stored as part of a container format. It should also be noted that LZO\u2019s license prevents it from being distributed with Hadoop and requires a separate install, unlike Snappy, which can be distributed with Hadoop. But snappy is fastest among all compression techniques.","title":"Lzo"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#gzip","text":"\u2022 Gzip provides very good compression performance (on average, about 2.5times the compression that\u2019d be offered by Snappy). \u2022 But in terms of processing speed its slow. \u2022 Gzip is also not splittable, so it should be used with a container format. \u2022 Note that one reason Gzip is sometimes slower than Snappy for processing is that Gzip compressed files take up fewer blocks, so fewer tasks are required for processing the same data. For this reason, using smaller blocks with Gzip can lead to better performance. \u2022 Eg \u2013 o 1 gb file \u2013 split into 8 blocks. o After gzip compression, we get 200 mb file \u2013 2 block \u2013 so number of blocks coming down, which reduces parallelism. o Solution \u2013 reduce block size to say, 50Mb, leading to 200mb file split into 4 blocks, and so parallelism doubles.","title":"Gzip"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#bzip2","text":"\u2022 Bzip2 provides excellent compression performance, but can be significantly slower than other compression codecs such as Snappy in terms of processing performance. \u2022 Unlike Snappy and Gzip, bzip2 is inherently splittable. \u2022 In the examples we have seen, bzip2 will normally compress around 9% better than GZip, in terms of storage space. \u2022 However, this extra compression comes with a significant read/write performance cost. This performance difference will vary with different machines, but in general bzip2 is about 10 times slower than GZip. \u2022 For this reason, it\u2019s not an ideal codec for Hadoop storage, unless your primary need is reducing the storage footprint. One example of such a use case would be using Hadoop mainly for active archival purposes. Codec Splittable compression speed snappy N low Highest lzo Y low high gzip N High slow bzip2 Y Highest Slowest","title":"Bzip2"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/","text":"Text File format Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text-based File Formats"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/#text-file-format","text":"Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text File format"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/","text":"Avro, ORC and Parquet There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Big Data File Formats"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/#avro-orc-and-parquet","text":"There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Avro, ORC and Parquet"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/","text":"Row Vs Column File Format When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row-based Vs Column-based File Formats"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/#row-vs-column-file-format","text":"When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row Vs Column File Format"}]}