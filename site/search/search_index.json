{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Big Data System Design Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture Topics Big Data Architectural Patterns 1. Introduction 2. Big Data Architectural Principles - A simplified data processing pipeline. - What is the temperature of my data? 3. Collect Layer - Types of data sources - Which Streaming/Message Storage should I use? - Which File/Object Storage should I use? - Data Tiering 4. Processing Layer","title":"Home"},{"location":"#welcome-to-big-data-system-design","text":"Author : Kushal Luthra repo url : https://github.com/kushal-luthra/big-data-architecture","title":"Welcome to Big Data System Design"},{"location":"#topics","text":"Big Data Architectural Patterns 1. Introduction 2. Big Data Architectural Principles - A simplified data processing pipeline. - What is the temperature of my data? 3. Collect Layer - Types of data sources - Which Streaming/Message Storage should I use? - Which File/Object Storage should I use? - Data Tiering 4. Processing Layer","title":"Topics"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/","text":"Big Data Analytics Architectural Patterns and Best Practices Topics -> - Big Data challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/AWSreInvent2018_Home/#big-data-analytics-architectural-patterns-and-best-practices","text":"Topics -> - Big Data challenges - Architectural Principles - How to simplify Big Data Processing - What Technologies you should use? - Why? - How? - Reference Architecture Patterns - Design Patterns source : - https://www.youtube.com/watch?v=ovPheIbY7U8 - https://www.youtube.com/watch?v=MotN5f6_xl8 - https://www.youtube.com/watch?v=nMyuCdqzpZc","title":"Big Data Analytics Architectural Patterns and Best Practices"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-1/","text":"There are different kind of analytical systems. Some customers are looking at doing batch-interactive analytics with their data. Others are looking at being able to to take real time data feeds, and store them for insights, and some go beyond, seking building of data models on top of it, power inferencing and do ML with that data. As we step step into the architecture discussion, we are going to discuss architecture for -> - Streaming processing - datalakes and batch interaction - machine learning. Note - you dont need to pick them right away. You can start small, architect it in a way that it enables you to build more and more features on top of it over time, incrementally. Now, lets look at different model of delivering big data services. Virtualized - eg- EC2 instances that we create & install kafka on top of it. Customer owns environment and manages it themselves. Managed Services - EMR (hadoop platform as a service), RDS - managed by AWS - customers still thinking about requirements like what configuration you need, what should be auto-scaling policy etc. Serverless/Clusterless/Containerized - Lambda, Athena, Glue - these are services that abstract out the servers away from you. Various services - both open source and AWS based are mentioned below - Going forward we will discuss -> * different kind of reference architectures. * what tools should one choose? * How? * Why?","title":"Introduction"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/","text":"Big Data Architectural Principles build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. This lets you be future-proof, lets you iterate, and lets you migrate over time. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares and other pieces. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages. A simplified data processing pipeline Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. Its not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders. What is the temperature of my data? Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#big-data-architectural-principles","text":"build loosely coupled or decoupled system. This holds true not just for Big Data Systems, but for any system. This means - the way I collect my data shouldn't be dependent on the way I analyze my data. So, I could change the way I collect my data by changing the tool and it shouldn't be impacting the way I store, process and/or analyze my data. This lets you be future-proof, lets you iterate, and lets you migrate over time. Being able to pick right tool for the right Job. If your system is loosely coupled, it gives you flexibility to pick right tool for right Job. Rather than trying to pick one tool to do everything, choose one which fits your use case. Eg- a. for streaming you an use Kafka, AWS Kineses b. for ML - sagemaker c. for storage - RDS, HDFS/S3 etc Leverage managed and serverless services - Not an architectural principle, but more of a recommendation. By leveraging managed and serverless services, you can focus on what really matters. It lets you focus on analytics, the transformations, the ETL, rather than loading softwares and other pieces. Use event-journaling design Patterns - It means as you are collecting data into big data systems its a good practice to not override your data. So, if you are getting data records, and some of those data records are getting corrected, then rather than correcting those records, keep appending to your dataset. Why - if you have large volume of data, and if there is ever an issue like -> i. your job has a bug, Or ii. you accidently delete your data, ...you have the option to replay history and regenerate your data. So, go for immutable datasets (data lake), materialized views. Be cost-conscious Lot of times, big data doesnt have to mean big cost. If you architect it correctly, say you are building a hadoop system and are decoupling storage and processing layers, you can build a very cost effective, performant system, and keep the cost down. Use ML to enable you applications. This is a growing trend wherein more and more companies are leveraging ML to build their competitive advantages.","title":"Big Data Architectural Principles"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#a-simplified-data-processing-pipeline","text":"Below, we see simplified data processing pipeline. Your exact use case may not match it, but you should look at logical constructs here. for example, in Collect layer, you need to ask questions like -> - How am I gonna capture and collect this information? - If I have different datasets, I may not be collecting and storing these datasets in the same way. - If I have GPS data or clickstream data, I would like to collect it differently from imagery or satellite data. You must also note there is a cycle here. Its not exactly a waterfall model. Often times you collect and store raw data, and that raw data is in original form like csv, json etc. Then you often times would like to take that raw data and create curated datasets - query optimized datasets to be able to very rapidly access that data. This could be through ML, Data warehousing etc. This is a iterative process wherein you take raw data, and pass it through various transformations processes, and convert it to normalized/de-normalized form in order for it to be consumed by different stakeholders.","title":"A simplified data processing pipeline"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-2/#what-is-the-temperature-of-my-data","text":"Often times we talk about temperature of your data, that means velocity of your data, your queries and your analytics. We will discuss about temperature across those spectrums.","title":"What is the temperature of my data?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/","text":"Collect Layer Types of data sources In collection phase, often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NOSql database. Often times these are records that you need to be able to analyze and process. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed. For stream storage, we have 3 main options -> Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2. Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard pcoesses a 1000 records/seconds. Say, you wanna scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. Amazon Kinesis Data Firehose - Managed data delivery - lets say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline procesing of that data. - here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be abel to store thet data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destionations in order to store the data as the data is flowing in. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second. Which Streaming/Message Storage should I use? SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage. Which File/Object Storage should I use? Here S3/HDFS are defacto standards. S3 allows you to build your data lake in a robust manner. You can run various analytics on it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute - no need to run compute clusters for storage ( unlike HDFS ) - S3 is cheap ~ Storage & EMR for compute - in HDFS, you need to keep cluster always on. - can run transient Amazon EMR clusters with Amazon EC2 spot instances. - multiple & heterogenous anlaysis clusters and servcies can use the same data. - consider a scenario wherein you ingest data into S3. Now since storage si separated from compute, youcan run different clustes on top of same source of data. For example, one load can be of spark on EMR(based on reserved instances), and other for GPUs (based on spot instances) Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. Security - it used encryption at rest and in transit both. Data Tiering Its about maintaining data in different tiers based on use case. Use of s3 implies no use of HDFS? No. One can store their working datasets like quick analysis intermediate data in HDFS for faster access. For example, say you to do some analysis on data that involves iterative reads, and here you can use HDFS storage on EMR cluster. AWS gives S3 analytics report that recommends which object should be placed in which tier. Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache - DynamoDB Accelerator Graph DB Amazon Neptune Key-Value document Amazon DynamoDB SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache. Which Store should I use? Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? What is the data structure? - Do I have a very fixed schema? - am I doing key-value lookups -> eg - we have tons of session data, and we can use dynamoDB or key-value store. - is it very very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Collect Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#collect-layer","text":"","title":"Collect Layer"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#types-of-data-sources","text":"In collection phase, often times you have different data sources -> - transactional databases like relational databases, MongoDB, or NOSql database. Often times these are records that you need to be able to analyze and process. We can call those Transactions - Similarly, you might have log data like Media files, Application Log files. These are large Files/Objects that you may be needing to store. - Finally, we have streaming data like device sensors, IoT platforms. These are Events data. Each of those collections methods often require different way of collecting of data. And, often times, storing these data is also different. For Transactional data, it usually goes into NoSQL or relational database. (we will discuss the criteria for this). For Files/Objects data, the defacto standard is HDFS/S3 - we need a big object store for which datalake based on HDFS/S3 is needed. For stream storage, we have 3 main options -> Apache Kafka - open source project. - well established. - High throughput distributed streaming platform. - so a client which wants to migrate their data can begin by moving over their kafka systems to ec2. Amazon Kinesis Data Streaming - Managed Stream storage - for example, here we define number of shards, and each shard pcoesses a 1000 records/seconds. Say, you wanna scale up to 100,000 records, you would only need to change the number of shards required, and not be worried about number of servers being used, etc. Amazon Kinesis Data Firehose - Managed data delivery - lets say you have streaming data, and instead of capturing real time insights, you want to capture that data, and do some sort of advanced processing or offline procesing of that data. - here AWS Kinesis Firehose comes into picture. It allows you configure an end point to be abel to store thet data. This could be S3 bucket (data lake) or Elastic Search (ELK Stack). This allows you to configure various destionations in order to store the data as the data is flowing in. In data streams you configure number of shards. In firehose, its purely based on amount of data that is sent though that pipe. - So, you dont have hvae to pre-provision, but there are soft limits to amount of data to be processed per second.","title":"Types of data sources"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-streamingmessage-storage-should-i-use","text":"SQS (Simple Queue Service) Vs Streaming Storage -> - If your use case if a simple producer and single consumer - go for SQS. - If yours is a case of complex architecture wherein you have multiple consumers and also want to store stream data, opt for Stream Storage.","title":"Which Streaming/Message Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-fileobject-storage-should-i-use","text":"Here S3/HDFS are defacto standards. S3 allows you to build your data lake in a robust manner. You can run various analytics on it. It is natively supported by wide number of tools including hadoop ecosystem like HDFS, presto, hive etc can talk to S3 to be able to read and process that data. Decouple Storage and Compute - no need to run compute clusters for storage ( unlike HDFS ) - S3 is cheap ~ Storage & EMR for compute - in HDFS, you need to keep cluster always on. - can run transient Amazon EMR clusters with Amazon EC2 spot instances. - multiple & heterogenous anlaysis clusters and servcies can use the same data. - consider a scenario wherein you ingest data into S3. Now since storage si separated from compute, youcan run different clustes on top of same source of data. For example, one load can be of spark on EMR(based on reserved instances), and other for GPUs (based on spot instances) Designed for 99.999999999 % durability (11 Nines). Tremendous data reliability. Data replication within the same region is done automatically. Security - it used encryption at rest and in transit both.","title":"Which File/Object Storage should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#data-tiering","text":"Its about maintaining data in different tiers based on use case. Use of s3 implies no use of HDFS? No. One can store their working datasets like quick analysis intermediate data in HDFS for faster access. For example, say you to do some analysis on data that involves iterative reads, and here you can use HDFS storage on EMR cluster. AWS gives S3 analytics report that recommends which object should be placed in which tier. Now, lets talk about databases. When it comes to databases, we have large number of options based on purpose. Purpose Database Caching - AWS elasticCache - DynamoDB Accelerator Graph DB Amazon Neptune Key-Value document Amazon DynamoDB SQL/RDBMS Amazon RDS (Relational Database Service) Amazon DynamoDB Accelerator (DAX) - its a dynamoDB Front end which has a write-through cache.","title":"Data Tiering"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-3/#which-store-should-i-use","text":"Choosing a right database involves asking right questions, including -> - what is the Data Structure ? - How will data be accessed? - What is the temperature of the data? - what will be solution cost? What is the data structure? - Do I have a very fixed schema? - am I doing key-value lookups -> eg - we have tons of session data, and we can use dynamoDB or key-value store. - is it very very relational data such that you are constantly traversing the graphs Also data access pattern is important. eg- - amazon aurora - very very high throughput transactional data need - if you need analytical capability of OLAP then use Amazon Redshift.","title":"Which Store should I use?"},{"location":"Big-data-architectural-patterns/Big-data-architectural-patterns-4/","text":"","title":"Big data architectural patterns 4"}]}